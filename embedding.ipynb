{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ab332f",
   "metadata": {},
   "source": [
    "## TOKEN EMBEDDINGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffea184",
   "metadata": {},
   "source": [
    "![Screenshot](images/screenshot3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d0f85",
   "metadata": {},
   "source": [
    "a well trained embedding can capture significant syntactical information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19969981",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Let's illustrate how the token ID to embedding vector conversion works with a hands-on\n",
    "example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "952d6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_ids = torch.tensor([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e47a2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of\n",
    "only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want\n",
    "to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ffae68",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch,\n",
    "setting the random seed to 123 for reproducibility purposes:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb058f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 4\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8e46182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880],\n",
       "        [ 0.3486,  0.6603, -0.2196, -0.3792],\n",
       "        [-0.1606, -0.4015,  0.6957, -1.8061],\n",
       "        [ 1.8960, -0.1750,  1.3689, -1.6033],\n",
       "        [-0.7849, -1.4096, -0.4076,  0.7953],\n",
       "        [ 0.9985,  0.2212,  1.8319, -0.3378]], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b4bea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "We can see that the weight matrix of the embedding layer contains small, random values.\n",
    "These values are optimized during LLM training as part of the LLM optimization itself, as we\n",
    "will see in upcoming chapters. Moreover, we can see that the weight matrix has six rows\n",
    "and three columns. There is one row for each of the six possible tokens in the vocabulary.\n",
    "And there is one column for each of the three embedding dimensions.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9df96b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "After we instantiated the embedding layer, let's now apply it to a token ID to obtain the\n",
    "embedding vector:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52f679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8960, -0.1750,  1.3689, -1.6033]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(torch.tensor([3])) # 4ht row of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d05549",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "each row in the embedding matrix is just an lookup to the token ids\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9975a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c456f66",
   "metadata": {},
   "source": [
    "![Screenshot](images/screenshot4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f39fd",
   "metadata": {},
   "source": [
    "this is after sinosuindal encoding, pick and 2 words which are close you'll see that most of the vector is same and for any two vectroo far away from each other the vectors are differnt , thus this is captuirinmg the relative positions pretty good, also if you see absolute positions of words are also captures in this as first word and last word are quite different (pick a line horizontally it represents positional encoding vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1979b",
   "metadata": {},
   "source": [
    "![Screenshot](images/screenshot5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b8616",
   "metadata": {},
   "source": [
    "**POSITIONAL EMBEDDINGS (ENCODING WORD POSITIONS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0349bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import tiktoken\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b61be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        #using sliding window to ceate input and target pairs\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i+1: max_length + 1 + i]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2808f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        pre_processed = re.split(r'([,.:;\"!?()\\'_]|--|\\s)', text)   \n",
    "        pre_processed = [item for item in pre_processed if item.strip()]\n",
    "        pre_processed = [item if item in self.str_to_int else \"<|unk|>\" for item in pre_processed]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in pre_processed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9de25085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85c3cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d7adde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wharton_verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d921e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size = 8,\n",
    "                                  max_length = max_length, stride = 2,\n",
    "                                  shuffle = True)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets  = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4235a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   13,  1002,   340,   547],\n",
      "        [  510,   379,   262,  4286],\n",
      "        [ 7109, 14655,   683,   866],\n",
      "        [  666,   966,    13,   383],\n",
      "        [ 1936,  2431,   438,   392],\n",
      "        [ 1917,    13,  1675, 24456],\n",
      "        [  464,   748,   586,   652],\n",
      "        [  526,   198, 43920,  3619]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffb0d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Embeddings:\n",
      " torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"\\nToken Embeddings:\\n\", token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f523449",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28eddb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "792eaec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947cbf51",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As shown in the preceding code example, the input to the pos_embeddings is usually a\n",
    "placeholder vector torch.arange(context_length), which contains a sequence of\n",
    "numbers 0, 1, ..., up to the maximum input length − 1. \n",
    "\n",
    "The context_length is a variable\n",
    "that represents the supported input size of the LLM. \n",
    "\n",
    "Here, we choose it similar to the\n",
    "maximum length of the input text. \n",
    "\n",
    "In practice, input text can be longer than the supported\n",
    "context length, in which case we have to truncate the text.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea5ae9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see, the positional embedding tensor consists of four 256-dimensional vectors.\n",
    "We can now add these directly to the token embeddings, where PyTorch will add the 4x256-\n",
    "dimensional pos_embeddings tensor to each 4x256-dimensional token embedding tensor in\n",
    "each of the 8 batches:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b75b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191a28ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
