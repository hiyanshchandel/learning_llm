{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ab332f",
   "metadata": {},
   "source": [
    "## TOKEN EMBEDDINGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffea184",
   "metadata": {},
   "source": [
    "![Screenshot](images/screenshot1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d0f85",
   "metadata": {},
   "source": [
    "a well trained embedding can capture significant syntactical information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19969981",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Let's illustrate how the token ID to embedding vector conversion works with a hands-on\n",
    "example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952d6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_ids = torch.tensor([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e47a2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of\n",
    "only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want\n",
    "to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ffae68",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch,\n",
    "setting the random seed to 123 for reproducibility purposes:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb058f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 4\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e46182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880],\n",
       "        [ 0.3486,  0.6603, -0.2196, -0.3792],\n",
       "        [-0.1606, -0.4015,  0.6957, -1.8061],\n",
       "        [ 1.8960, -0.1750,  1.3689, -1.6033],\n",
       "        [-0.7849, -1.4096, -0.4076,  0.7953],\n",
       "        [ 0.9985,  0.2212,  1.8319, -0.3378]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b4bea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "We can see that the weight matrix of the embedding layer contains small, random values.\n",
    "These values are optimized during LLM training as part of the LLM optimization itself, as we\n",
    "will see in upcoming chapters. Moreover, we can see that the weight matrix has six rows\n",
    "and three columns. There is one row for each of the six possible tokens in the vocabulary.\n",
    "And there is one column for each of the three embedding dimensions.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9df96b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "After we instantiated the embedding layer, let's now apply it to a token ID to obtain the\n",
    "embedding vector:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff52f679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8960, -0.1750,  1.3689, -1.6033]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(torch.tensor([3])) # 4ht row of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d05549",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "each row in the embedding matrix is just an lookup to the token ids\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9975a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c456f66",
   "metadata": {},
   "source": [
    "![Screenshot](images/screenshot4.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f39fd",
   "metadata": {},
   "source": [
    "this is after sinosuindal encoding, pick and 2 words which are close you'll see that most of the vector is same and for any two vectroo far away from each other the vectors are differnt , thus this is captuirinmg the relative positions pretty good, also if you see absolute positions of words are also captures in this as first word and last word are quite different (pick a line horizontally it represents positional encoding vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1979b",
   "metadata": {},
   "source": [
    "![Screenshot](images/screenshot5.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b8616",
   "metadata": {},
   "source": [
    "**POSITIONAL EMBEDDINGS (ENCODING WORD POSITIONS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0349bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import tiktoken\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b61be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        #using sliding window to ceate input and target pairs\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i+1: max_length + 1 + i]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2808f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        pre_processed = re.split(r'([,.:;\"!?()\\'_]|--|\\s)', text)   \n",
    "        pre_processed = [item for item in pre_processed if item.strip()]\n",
    "        pre_processed = [item if item in self.str_to_int else \"<|unk|>\" for item in pre_processed]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in pre_processed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9de25085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85c3cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d7adde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wharton_verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d921e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size = 8,\n",
    "                                  max_length = max_length, stride = 2,\n",
    "                                  shuffle = True)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets  = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4235a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[  257,  1207,  8344,   803],\n",
      "        [  625,   262, 24818,   417],\n",
      "        [  257,  6487,    13,   366],\n",
      "        [  198,  1544, 13818,  4622],\n",
      "        [35569,   502,    13,   887],\n",
      "        [  683,    11, 10597,   314],\n",
      "        [  198,  1870,   465,  8216],\n",
      "        [  198,   265,  6384,  1456]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffb0d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Embeddings:\n",
      " torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"\\nToken Embeddings:\\n\", token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f523449",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28eddb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "792eaec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947cbf51",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As shown in the preceding code example, the input to the pos_embeddings is usually a\n",
    "placeholder vector torch.arange(context_length), which contains a sequence of\n",
    "numbers 0, 1, ..., up to the maximum input length − 1. \n",
    "\n",
    "The context_length is a variable\n",
    "that represents the supported input size of the LLM. \n",
    "\n",
    "Here, we choose it similar to the\n",
    "maximum length of the input text. \n",
    "\n",
    "In practice, input text can be longer than the supported\n",
    "context length, in which case we have to truncate the text.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea5ae9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see, the positional embedding tensor consists of four 256-dimensional vectors.\n",
    "We can now add these directly to the token embeddings, where PyTorch will add the 4x256-\n",
    "dimensional pos_embeddings tensor to each 4x256-dimensional token embedding tensor in\n",
    "each of the 8 batches:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5b75b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39297c36",
   "metadata": {},
   "source": [
    "## IMPLEMENTING A COMPACT SELF ATTENTION PYTHON CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c2f45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_K = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_V = nn.Parameter(torch.randn(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_K\n",
    "        queries = x @ self.W_Q\n",
    "        values = x @ self.W_V\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5 , dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ab1d396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7053, -0.5213,  0.7561, -1.1033,  0.0726],\n",
      "        [ 0.6443, -0.5202,  0.7027, -1.0369,  0.1176],\n",
      "        [ 0.6507, -0.5203,  0.7091, -1.0447,  0.1130],\n",
      "        [ 0.6933, -0.5269,  0.7448, -1.0918,  0.0881],\n",
      "        [ 0.7910, -0.5222,  0.8522, -1.2159,  0.0105],\n",
      "        [ 0.6292, -0.5259,  0.6801, -1.0142,  0.1349]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "torch.manual_seed(123) \n",
    "\n",
    "sa_v1 = SelfAttention_v1(3,5)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b62737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_K = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_V = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_K(x)\n",
    "        queries = self.W_Q(x)\n",
    "        values = self.W_V(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5 , dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb0fea03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v1 = SelfAttention_v2(3,2)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a7095",
   "metadata": {},
   "source": [
    "## HIDING FUTURE WORDS WITH CAUSAL ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1b20967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = inputs.shape[0]\n",
    "torch.triu(torch.ones(context_length, context_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d788eb",
   "metadata": {},
   "source": [
    "### IMPLEMENTING A COMPACT CAUSAL ATTENTION CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e309b9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In this section, we will now incorporate the causal attention and dropout modifications into\n",
    "the SelfAttention Python class we developed in section 3.4. \n",
    "\n",
    "This class will then serve as a\n",
    "template for developing multi-head attention in the upcoming section.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8b611d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Before we begin, one more thing is to ensure that the code can handle batches\n",
    "consisting of more than one input. \n",
    "\n",
    "This will ensure that the CausalAttention class supports the batch\n",
    "outputs produced by the data loader we implemented earlier.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e96154",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "For simplicity, to simulate such batch inputs, we duplicate the input text example:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598e083",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    " 2 inputs with 6 tokens each, and each token has embedding dimension 3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65378ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    "                )\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84968b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length,context_length),diagonal = 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens , d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1,2)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "        attn_weights = torch.softmax(attn_scores /keys.shape[-1]**0.5, dim = 1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7560ba2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.1081,  0.1840, -0.1018],\n",
       "         [ 0.1729,  0.2798, -0.0797],\n",
       "         [ 0.2621,  0.4295, -0.1738],\n",
       "         [ 0.3809,  0.5929, -0.1858],\n",
       "         [ 0.4447,  0.7247, -0.2576]],\n",
       "\n",
       "        [[ 0.0997,  0.1696, -0.0939],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.2518,  0.4119, -0.1638],\n",
       "         [ 0.2621,  0.4295, -0.1738],\n",
       "         [ 0.0951,  0.3614, -0.2786],\n",
       "         [ 0.4420,  0.6948, -0.2183]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CasualAttention(3, 3, context_length, dropout=0.5)\n",
    "context_vec = ca(batch)\n",
    "context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613e4ff",
   "metadata": {},
   "source": [
    "## EXTENDING SINGLE HEAD ATTENTION TO STACKED ATTENTION (MULTIHEAD ATTENTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71eb2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98264cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAtttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            CasualAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4653dbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0499,  0.0848, -0.0469,  0.0143,  0.0871,  0.0520],\n",
       "         [ 0.1262,  0.2064, -0.0820,  0.0101,  0.2142,  0.1086],\n",
       "         [ 0.2124,  0.3458, -0.1218,  0.0081,  0.3906,  0.1888],\n",
       "         [ 0.2885,  0.4546, -0.1387,  0.0028,  0.5420,  0.2603],\n",
       "         [ 0.2615,  0.5105, -0.2161,  0.0027,  0.7399,  0.3124],\n",
       "         [ 0.6519,  0.9957, -0.2431, -0.0180,  1.2439,  0.5864]],\n",
       "\n",
       "        [[ 0.0499,  0.0848, -0.0469,  0.0143,  0.0871,  0.0520],\n",
       "         [ 0.1262,  0.2064, -0.0820,  0.0101,  0.2142,  0.1086],\n",
       "         [ 0.2124,  0.3458, -0.1218,  0.0081,  0.3906,  0.1888],\n",
       "         [ 0.2885,  0.4546, -0.1387,  0.0028,  0.5420,  0.2603],\n",
       "         [ 0.2615,  0.5105, -0.2161,  0.0027,  0.7399,  0.3124],\n",
       "         [ 0.6519,  0.9957, -0.2431, -0.0180,  1.2439,  0.5864]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "mha = MultiHeadAtttentionWrapper(3, 3, context_length, dropout=0, num_heads = 2)\n",
    "context_vec = mha(batch)\n",
    "context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac42a33",
   "metadata": {},
   "source": [
    "### BATCHED STACKED MULTI HEADED ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a73a18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0) , \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_Value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # same as W_O\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens , d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_Value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = (attn_weights @ values).transpose(1,2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c8e16",
   "metadata": {},
   "source": [
    "![Screenshot](images/screenshot6.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "464ca14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
      "\n",
      "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the tensor with 3 rows and 6 columns\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) \n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 6\n",
    "mha = MultiHeadedAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a7ec0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention\n",
    "heads and a context vector embedding size of 768. \n",
    "\n",
    "The largest GPT-2 model (1.5 billion\n",
    "parameters) has 25 attention heads and a context vector embedding size of 1600.\n",
    "\n",
    "Note\n",
    "that the embedding sizes of the token inputs and context embeddings are the same in GPT\n",
    "models (d_in = d_out).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053efc83",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 1: DUMMY GPT MODEL CLASS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf58d2f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: Use a placeholder for TransformerBlock\n",
    "\n",
    "Step 2: Use a placeholder for LayerNorm\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98f15783",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73ff8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = GPT_CONFIG_124M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccb83c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.dropout(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg): \n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20866762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c966b88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6f9f5",
   "metadata": {},
   "source": [
    "## LAYER NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80a99f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a38419a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The neural network layer we have coded consists of a Linear layer followed by a non-linear\n",
    "activation function, ReLU (short for Rectified Linear Unit), which is a standard activation\n",
    "function in neural networks. \n",
    "\n",
    "If you are unfamiliar with ReLU, it simply thresholds negative\n",
    "inputs to 0, ensuring that a layer outputs only positive values, which explains why the\n",
    "resulting layer output does not contain any negative values. \n",
    "\n",
    "(Note that we will use another,\n",
    "more sophisticated activation function in GPT, which we will introduce in the next section).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e7a81e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim = True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7709636",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The first row in the mean tensor above contains the mean value for the first input row, and\n",
    "the second output row contains the mean for the second input row.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d92a06e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer output\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[-5.9605e-08],\n",
      "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim = -1, keepdim = True)\n",
    "var = out_norm.var(dim = -1, keepdim = True)\n",
    "print(\"Normalized layer output\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0831a4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Note that the value 2.9802e-08 in the output tensor is the scientific notation for 2.9802 ×\n",
    "10-8, which is 0.0000000298 in decimal form. This value is very close to 0, but it is not\n",
    "exactly 0 due to small numerical errors that can accumulate because of the finite precision\n",
    "with which computers represent numbers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8af0f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False)\n",
    "        norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afd80a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "This specific implementation of layer Normalization operates on the last dimension of the\n",
    "input tensor x, which represents the embedding dimension (emb_dim). \n",
    "\n",
    "The variable eps is a\n",
    "small constant (epsilon) added to the variance to prevent division by zero during\n",
    "normalization. \n",
    "\n",
    "The scale and shift are two trainable parameters (of the same dimension\n",
    "as the input) that the LLM automatically adjusts during training if it is determined that\n",
    "doing so would improve the model's performance on its training task. \n",
    "\n",
    "This allows the model\n",
    "to learn appropriate scaling and shifting that best suit the data it is processing.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a11e0b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "In our variance calculation method, we have opted for an implementation detail by\n",
    "setting unbiased=False. \n",
    "\n",
    "For those curious about what this means, in the variance\n",
    "calculation, we divide by the number of inputs n in the variance formula. \n",
    "\n",
    "This approach does not apply Bessel's correction, which typically uses n-1 instead of n in\n",
    "the denominator to adjust for bias in sample variance estimation. \n",
    "\n",
    "This decision results in a so-called biased estimate of the variance. \n",
    "\n",
    "For large-scale language\n",
    "models (LLMs), where the embedding dimension n is significantly large, the\n",
    "difference between using n and n-1 is practically negligible. \n",
    "\n",
    "We chose this approach to ensure compatibility with the GPT-2 model's normalization layers and because it\n",
    "reflects TensorFlow's default behavior, which was used to implement the original GPT2 model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c30075c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
      "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n",
      "Mean:\n",
      " tensor([[-2.9802e-08],\n",
      "        [ 0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim = 5)\n",
    "out_ln  = ln(batch_example)\n",
    "print(out_ln)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81a5cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e01bb900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu, y_gelu_dist = gelu(x), relu(x), (gelu(x))/x\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu,y_gelu_dist], [\"GELU\", \"ReLU\", \"GELU_DIST\"]), 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c365cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),    \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ef2300c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768) #A\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8338b9",
   "metadata": {},
   "source": [
    "# SHORTCUT CONNECTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b697c61",
   "metadata": {},
   "source": [
    "![Screenshot](images/screenshot7.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f747b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e63ad",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The code implements a deep neural network with 5 layers, each consisting of a Linear\n",
    "layer and a GELU activation function. \n",
    "\n",
    "In the forward pass, we iteratively pass the input\n",
    "through the layers and optionally add the shortcut connections  if\n",
    "the self.use_shortcut attribute is set to True.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850696e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's use this code to first initialize a neural network without shortcut connections. Here,\n",
    "each layer will be initialized such that it accepts an example with 3 input values and returns\n",
    "3 output values. The last layer returns a single output value:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb4932ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # specify random seed for the initial weights for reproducibility\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02063c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63d59bd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In the preceding code, we specify a loss function that computes how close the model output\n",
    "and a user-specified target (here, for simplicity, the value 0) are. \n",
    "\n",
    "Then, when calling loss.backward(), PyTorch computes the loss gradient for each layer in the model. \n",
    "\n",
    "We can iterate through the weight parameters via model.named_parameters(). \n",
    "\n",
    "Suppose we have a 3×3 weight parameter matrix for a given layer. \n",
    "\n",
    "In that case, this layer will have 3×3 gradient values, and we print the mean absolute gradient of these 3×3 gradient values to\n",
    "obtain a single gradient value per layer to compare the gradients between layers more\n",
    "easily.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd40f4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In short, the .backward() method is a convenient method in PyTorch that computes loss\n",
    "gradients, which are required during model training, without implementing the math for the\n",
    "gradient calculation ourselves, thereby making working with deep neural networks much\n",
    "more accessible. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9b839",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now use the print_gradients function and apply it to the model without skip\n",
    "connections:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cdca392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152039906941354\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed95d82",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see based on the output of the print_gradients function, the gradients become\n",
    "smaller as we progress from the last layer (layers.4) to the first layer (layers.0), which\n",
    "is a phenomenon called the vanishing gradient problem.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa92b4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's now instantiate a model with skip connections and see how it compares:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7194e3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a1472",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see, based on the output, the last layer (layers.4) still has a larger gradient\n",
    "than the other layers. \n",
    "\n",
    "However, the gradient value stabilizes as we progress towards the\n",
    "first layer (layers.0) and doesn't shrink to a vanishingly small value.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd87349",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In conclusion, shortcut connections are important for overcoming the limitations posed\n",
    "by the vanishing gradient problem in deep neural networks. \n",
    "\n",
    "Shortcut connections are a core building block of very large models such as LLMs, and they will help facilitate more effective\n",
    "training by ensuring consistent gradient flow across layers when we train the GPT model \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0355764",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 5: CODING ATTENTION AND LINEAR LAYERS IN A TRANSFORMER BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1cdc2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False)\n",
    "        norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),    \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aef78877",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransofrmerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadedAttention(\n",
    "            cfg[\"emb_dim\"], cfg[\"emb_dim\"], cfg[\"context_length\"],\n",
    "            cfg[\"drop_rate\"], cfg[\"n_heads\"], qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward (self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268da556",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Layer normalization (LayerNorm) is applied before each of these two components, and\n",
    "dropout is applied after them to regularize the model and prevent overfitting. \n",
    "\n",
    "This is also known as Pre-LayerNorm. \n",
    "\n",
    "Older architectures, such as the original transformer model,\n",
    "applied layer normalization after the self-attention and feed-forward networks instead,\n",
    "known as Post-LayerNorm, which often leads to worse training dynamics.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b3de627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransofrmerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db003c40",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see from the code output, the transformer block maintains the input dimensions\n",
    "in its output, indicating that the transformer architecture processes sequences of data\n",
    "without altering their shape throughout the network.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b11ebb9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "The preservation of shape throughout the transformer block architecture is not incidental\n",
    "but a crucial aspect of its design. \n",
    "\n",
    "This design enables its effective application across a wide\n",
    "range of sequence-to-sequence tasks, where each output vector directly corresponds to an\n",
    "input vector, maintaining a one-to-one relationship. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d69f799",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "However, the output is a context vector\n",
    "that encapsulates information from the entire input sequence.\n",
    "\n",
    "This means that while the physical dimensions of the sequence (length and feature size)\n",
    "remain unchanged as it passes through the transformer block, the content of each output\n",
    "vector is re-encoded to integrate contextual information from across the entire input\n",
    "sequence.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403551c2",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 6: ENTIRE GPT MODEL ARCHITECTURE IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941cd900",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The device setting will allow us to train the model on a CPU or GPU, depending on which device the input\n",
    "data sits\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b28e6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb  = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransofrmerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ef2fb85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76e164",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, the output tensor has the shape [2, 4, 50257], since we passed in 2 input\n",
    "texts with 4 tokens each. The last dimension, 50,257, corresponds to the vocabulary size of\n",
    "the tokenizer. In the next section, we will see how to convert each of these 50,257-\n",
    "dimensional output vectors back into tokens.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b134d41",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Using the numel() method, short for \"number of elements,\" we can collect the total\n",
    "number of parameters in the model's parameter tensors:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b25de605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011a00b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Earlier, we spoke of initializing a 124\n",
    "million parameter GPT model, so why is the actual number of parameters 163 million, as\n",
    "shown in the preceding code output?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3235865",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "The reason is a concept called weight tying that is used in the original GPT-2\n",
    "architecture, which means that the original GPT-2 architecture is reusing the weights from\n",
    "the token embedding layer in its output layer. \n",
    "\n",
    "To understand what this means, let's take a\n",
    "look at the shapes of the token embedding layer and linear output layer that we initialized\n",
    "on the model via the GPTModel earlier:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d7cfbf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed42f56",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see based on the print outputs, the weight tensors for both these layers have the\n",
    "same shape:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350bb954",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The token embedding and output layers are very large due to the number of rows for the\n",
    "50,257 in the tokenizer's vocabulary. Let's remove the output layer parameter count from\n",
    "the total GPT-2 model count according to the weight tying:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "38117b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2151f226",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, the model is now only 124 million parameters large, matching the original\n",
    "size of the GPT-2 model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7752dd4b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "Weight tying reduces the overall memory footprint and computational complexity of the\n",
    "model. However, in my experience, using separate token embedding and output layers\n",
    "results in better training and model performance; hence, we are using separate layers in\n",
    "our GPTModel implementation. The same is true for modern LLMs.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a3397b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Lastly, let us compute the memory requirements of the 163 million parameters in our\n",
    "GPTModel object:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e4c8f883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4 #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e640d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "In conclusion, by calculating the memory requirements for the 163 million parameters in\n",
    "our GPTModel object and assuming each parameter is a 32-bit float taking up 4 bytes, we\n",
    "find that the total size of the model amounts to 621.83 MB, illustrating the relatively large\n",
    "storage capacity required to accommodate even relatively small LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f28e64",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "In this section, we implemented the GPTModel architecture and saw that it outputs\n",
    "numeric tensors of shape [batch_size, num_tokens, vocab_size]. In the next section,\n",
    "we will write the code to convert these output tensors into text.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf9a9d",
   "metadata": {},
   "source": [
    "Weight tying means sharing the same weight matrix between:\n",
    "\n",
    "The token embedding layer (converts token IDs to vectors), and\n",
    "\n",
    "The output projection layer (converts final vectors to logits over the vocabulary).\n",
    "\n",
    "So instead of having two separate matrices:\n",
    "\n",
    "E for input embedding: shape [vocab_size, emb_dim]\n",
    "\n",
    "W for output projection: shape [emb_dim, vocab_size]\n",
    "\n",
    "We reuse E.T as the output layer:\n",
    "\n",
    "logits = x @ E.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e34fb",
   "metadata": {},
   "source": [
    " Why Use Weight Tying?\n",
    "Fewer Parameters\n",
    "\n",
    "GPT-2 has a huge vocabulary (e.g., 50,000 tokens). Instead of storing two large matrices, weight tying saves memory.\n",
    "\n",
    "Improved Generalization\n",
    "\n",
    "Sharing weights forces the model to treat input and output tokens in a more consistent way.\n",
    "\n",
    "This helps especially in language modeling, where the same vocabulary is used at both ends.\n",
    "\n",
    "Empirical Gains\n",
    "\n",
    "Papers like \"Using the Output Embedding to Improve Language Models\" (Press & Wolf, 2017) showed that weight tying improves perplexity.\n",
    "\n",
    "🧠 Intuition\n",
    "Think of it like this:\n",
    "\n",
    "If the embedding for \"cat\" is a certain vector, then the output layer should also understand that same vector means \"cat\".\n",
    "\n",
    "Sharing weights enforces this symmetry: the model learns one set of weights that both understands and generates token representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82bb33a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
