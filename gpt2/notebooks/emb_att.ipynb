{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ab332f",
   "metadata": {},
   "source": [
    "## TOKEN EMBEDDINGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffea184",
   "metadata": {},
   "source": [
    "![Screenshot](images/screenshot1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d0f85",
   "metadata": {},
   "source": [
    "a well trained embedding can capture significant syntactical information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19969981",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Let's illustrate how the token ID to embedding vector conversion works with a hands-on\n",
    "example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "952d6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_ids = torch.tensor([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e47a2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of\n",
    "only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want\n",
    "to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ffae68",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch,\n",
    "setting the random seed to 123 for reproducibility purposes:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb058f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 4\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e46182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880],\n",
       "        [ 0.3486,  0.6603, -0.2196, -0.3792],\n",
       "        [-0.1606, -0.4015,  0.6957, -1.8061],\n",
       "        [ 1.8960, -0.1750,  1.3689, -1.6033],\n",
       "        [-0.7849, -1.4096, -0.4076,  0.7953],\n",
       "        [ 0.9985,  0.2212,  1.8319, -0.3378]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b4bea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "We can see that the weight matrix of the embedding layer contains small, random values.\n",
    "These values are optimized during LLM training as part of the LLM optimization itself, as we\n",
    "will see in upcoming chapters. Moreover, we can see that the weight matrix has six rows\n",
    "and three columns. There is one row for each of the six possible tokens in the vocabulary.\n",
    "And there is one column for each of the three embedding dimensions.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9df96b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "After we instantiated the embedding layer, let's now apply it to a token ID to obtain the\n",
    "embedding vector:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff52f679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8960, -0.1750,  1.3689, -1.6033]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(torch.tensor([3])) # 4ht row of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d05549",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "each row in the embedding matrix is just an lookup to the token ids\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9975a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c456f66",
   "metadata": {},
   "source": [
    "![Screenshot](images/screenshot4.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f39fd",
   "metadata": {},
   "source": [
    "this is after sinosuindal encoding, pick and 2 words which are close you'll see that most of the vector is same and for any two vectroo far away from each other the vectors are differnt , thus this is captuirinmg the relative positions pretty good, also if you see absolute positions of words are also captures in this as first word and last word are quite different (pick a line horizontally it represents positional encoding vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1979b",
   "metadata": {},
   "source": [
    "![Screenshot](images/screenshot5.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b8616",
   "metadata": {},
   "source": [
    "**POSITIONAL EMBEDDINGS (ENCODING WORD POSITIONS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0349bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import tiktoken\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b61be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        #using sliding window to ceate input and target pairs\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i+1: max_length + 1 + i]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2808f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        pre_processed = re.split(r'([,.:;\"!?()\\'_]|--|\\s)', text)   \n",
    "        pre_processed = [item for item in pre_processed if item.strip()]\n",
    "        pre_processed = [item if item in self.str_to_int else \"<|unk|>\" for item in pre_processed]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in pre_processed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9de25085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85c3cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d7adde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wharton_verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d921e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size = 8,\n",
    "                                  max_length = max_length, stride = 2,\n",
    "                                  shuffle = True)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets  = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4235a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[  257,  1207,  8344,   803],\n",
      "        [  625,   262, 24818,   417],\n",
      "        [  257,  6487,    13,   366],\n",
      "        [  198,  1544, 13818,  4622],\n",
      "        [35569,   502,    13,   887],\n",
      "        [  683,    11, 10597,   314],\n",
      "        [  198,  1870,   465,  8216],\n",
      "        [  198,   265,  6384,  1456]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffb0d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token Embeddings:\n",
      " torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"\\nToken Embeddings:\\n\", token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f523449",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28eddb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "792eaec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947cbf51",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As shown in the preceding code example, the input to the pos_embeddings is usually a\n",
    "placeholder vector torch.arange(context_length), which contains a sequence of\n",
    "numbers 0, 1, ..., up to the maximum input length − 1. \n",
    "\n",
    "The context_length is a variable\n",
    "that represents the supported input size of the LLM. \n",
    "\n",
    "Here, we choose it similar to the\n",
    "maximum length of the input text. \n",
    "\n",
    "In practice, input text can be longer than the supported\n",
    "context length, in which case we have to truncate the text.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea5ae9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see, the positional embedding tensor consists of four 256-dimensional vectors.\n",
    "We can now add these directly to the token embeddings, where PyTorch will add the 4x256-\n",
    "dimensional pos_embeddings tensor to each 4x256-dimensional token embedding tensor in\n",
    "each of the 8 batches:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5b75b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39297c36",
   "metadata": {},
   "source": [
    "## IMPLEMENTING A COMPACT SELF ATTENTION PYTHON CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c2f45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_K = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_V = nn.Parameter(torch.randn(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_K\n",
    "        queries = x @ self.W_Q\n",
    "        values = x @ self.W_V\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5 , dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ab1d396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7053, -0.5213,  0.7561, -1.1033,  0.0726],\n",
      "        [ 0.6443, -0.5202,  0.7027, -1.0369,  0.1176],\n",
      "        [ 0.6507, -0.5203,  0.7091, -1.0447,  0.1130],\n",
      "        [ 0.6933, -0.5269,  0.7448, -1.0918,  0.0881],\n",
      "        [ 0.7910, -0.5222,  0.8522, -1.2159,  0.0105],\n",
      "        [ 0.6292, -0.5259,  0.6801, -1.0142,  0.1349]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "torch.manual_seed(123) \n",
    "\n",
    "sa_v1 = SelfAttention_v1(3,5)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b62737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_K = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_V = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_K(x)\n",
    "        queries = self.W_Q(x)\n",
    "        values = self.W_V(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5 , dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb0fea03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v1 = SelfAttention_v2(3,2)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a7095",
   "metadata": {},
   "source": [
    "## HIDING FUTURE WORDS WITH CAUSAL ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1b20967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = inputs.shape[0]\n",
    "torch.triu(torch.ones(context_length, context_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d788eb",
   "metadata": {},
   "source": [
    "### IMPLEMENTING A COMPACT CAUSAL ATTENTION CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e309b9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In this section, we will now incorporate the causal attention and dropout modifications into\n",
    "the SelfAttention Python class we developed in section 3.4. \n",
    "\n",
    "This class will then serve as a\n",
    "template for developing multi-head attention in the upcoming section.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8b611d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Before we begin, one more thing is to ensure that the code can handle batches\n",
    "consisting of more than one input. \n",
    "\n",
    "This will ensure that the CausalAttention class supports the batch\n",
    "outputs produced by the data loader we implemented earlier.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e96154",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "For simplicity, to simulate such batch inputs, we duplicate the input text example:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598e083",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    " 2 inputs with 6 tokens each, and each token has embedding dimension 3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65378ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    "                )\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84968b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length,context_length),diagonal = 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens , d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1,2)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "        attn_weights = torch.softmax(attn_scores /keys.shape[-1]**0.5, dim = 1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7560ba2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.1081,  0.1840, -0.1018],\n",
       "         [ 0.1729,  0.2798, -0.0797],\n",
       "         [ 0.2621,  0.4295, -0.1738],\n",
       "         [ 0.3809,  0.5929, -0.1858],\n",
       "         [ 0.4447,  0.7247, -0.2576]],\n",
       "\n",
       "        [[ 0.0997,  0.1696, -0.0939],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.2518,  0.4119, -0.1638],\n",
       "         [ 0.2621,  0.4295, -0.1738],\n",
       "         [ 0.0951,  0.3614, -0.2786],\n",
       "         [ 0.4420,  0.6948, -0.2183]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CasualAttention(3, 3, context_length, dropout=0.5)\n",
    "context_vec = ca(batch)\n",
    "context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613e4ff",
   "metadata": {},
   "source": [
    "## EXTENDING SINGLE HEAD ATTENTION TO STACKED ATTENTION (MULTIHEAD ATTENTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71eb2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98264cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAtttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            CasualAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4653dbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0499,  0.0848, -0.0469,  0.0143,  0.0871,  0.0520],\n",
       "         [ 0.1262,  0.2064, -0.0820,  0.0101,  0.2142,  0.1086],\n",
       "         [ 0.2124,  0.3458, -0.1218,  0.0081,  0.3906,  0.1888],\n",
       "         [ 0.2885,  0.4546, -0.1387,  0.0028,  0.5420,  0.2603],\n",
       "         [ 0.2615,  0.5105, -0.2161,  0.0027,  0.7399,  0.3124],\n",
       "         [ 0.6519,  0.9957, -0.2431, -0.0180,  1.2439,  0.5864]],\n",
       "\n",
       "        [[ 0.0499,  0.0848, -0.0469,  0.0143,  0.0871,  0.0520],\n",
       "         [ 0.1262,  0.2064, -0.0820,  0.0101,  0.2142,  0.1086],\n",
       "         [ 0.2124,  0.3458, -0.1218,  0.0081,  0.3906,  0.1888],\n",
       "         [ 0.2885,  0.4546, -0.1387,  0.0028,  0.5420,  0.2603],\n",
       "         [ 0.2615,  0.5105, -0.2161,  0.0027,  0.7399,  0.3124],\n",
       "         [ 0.6519,  0.9957, -0.2431, -0.0180,  1.2439,  0.5864]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "mha = MultiHeadAtttentionWrapper(3, 3, context_length, dropout=0, num_heads = 2)\n",
    "context_vec = mha(batch)\n",
    "context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac42a33",
   "metadata": {},
   "source": [
    "### BATCHED STACKED MULTI HEADED ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a73a18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0) , \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_Value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # same as W_O\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens , d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_Value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = (attn_weights @ values).transpose(1,2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c8e16",
   "metadata": {},
   "source": [
    "![Screenshot](images/screenshot6.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "464ca14c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Define the tensor with 3 rows and 6 columns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m      5\u001b[0m     [[\u001b[38;5;241m0.43\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.89\u001b[39m, \u001b[38;5;241m0.55\u001b[39m, \u001b[38;5;241m0.87\u001b[39m, \u001b[38;5;241m0.66\u001b[39m],  \u001b[38;5;66;03m# Row 1\u001b[39;00m\n\u001b[1;32m      6\u001b[0m      [\u001b[38;5;241m0.57\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0.64\u001b[39m, \u001b[38;5;241m0.22\u001b[39m, \u001b[38;5;241m0.58\u001b[39m, \u001b[38;5;241m0.33\u001b[39m],  \u001b[38;5;66;03m# Row 2\u001b[39;00m\n\u001b[1;32m      7\u001b[0m      [\u001b[38;5;241m0.77\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.10\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.80\u001b[39m, \u001b[38;5;241m0.55\u001b[39m]]  \u001b[38;5;66;03m# Row 3\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the tensor with 3 rows and 6 columns\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) \n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 6\n",
    "mha = MultiHeadedAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a7ec0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention\n",
    "heads and a context vector embedding size of 768. \n",
    "\n",
    "The largest GPT-2 model (1.5 billion\n",
    "parameters) has 25 attention heads and a context vector embedding size of 1600.\n",
    "\n",
    "Note\n",
    "that the embedding sizes of the token inputs and context embeddings are the same in GPT\n",
    "models (d_in = d_out).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053efc83",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 1: DUMMY GPT MODEL CLASS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf58d2f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: Use a placeholder for TransformerBlock\n",
    "\n",
    "Step 2: Use a placeholder for LayerNorm\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98f15783",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73ff8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = GPT_CONFIG_124M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ccb83c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.dropout(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg): \n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20866762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c966b88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6f9f5",
   "metadata": {},
   "source": [
    "## LAYER NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80a99f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a38419a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The neural network layer we have coded consists of a Linear layer followed by a non-linear\n",
    "activation function, ReLU (short for Rectified Linear Unit), which is a standard activation\n",
    "function in neural networks. \n",
    "\n",
    "If you are unfamiliar with ReLU, it simply thresholds negative\n",
    "inputs to 0, ensuring that a layer outputs only positive values, which explains why the\n",
    "resulting layer output does not contain any negative values. \n",
    "\n",
    "(Note that we will use another,\n",
    "more sophisticated activation function in GPT, which we will introduce in the next section).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e7a81e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim = True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7709636",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The first row in the mean tensor above contains the mean value for the first input row, and\n",
    "the second output row contains the mean for the second input row.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d92a06e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer output\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[-5.9605e-08],\n",
      "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim = -1, keepdim = True)\n",
    "var = out_norm.var(dim = -1, keepdim = True)\n",
    "print(\"Normalized layer output\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0831a4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Note that the value 2.9802e-08 in the output tensor is the scientific notation for 2.9802 ×\n",
    "10-8, which is 0.0000000298 in decimal form. This value is very close to 0, but it is not\n",
    "exactly 0 due to small numerical errors that can accumulate because of the finite precision\n",
    "with which computers represent numbers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8af0f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False)\n",
    "        norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afd80a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "This specific implementation of layer Normalization operates on the last dimension of the\n",
    "input tensor x, which represents the embedding dimension (emb_dim). \n",
    "\n",
    "The variable eps is a\n",
    "small constant (epsilon) added to the variance to prevent division by zero during\n",
    "normalization. \n",
    "\n",
    "The scale and shift are two trainable parameters (of the same dimension\n",
    "as the input) that the LLM automatically adjusts during training if it is determined that\n",
    "doing so would improve the model's performance on its training task. \n",
    "\n",
    "This allows the model\n",
    "to learn appropriate scaling and shifting that best suit the data it is processing.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a11e0b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "In our variance calculation method, we have opted for an implementation detail by\n",
    "setting unbiased=False. \n",
    "\n",
    "For those curious about what this means, in the variance\n",
    "calculation, we divide by the number of inputs n in the variance formula. \n",
    "\n",
    "This approach does not apply Bessel's correction, which typically uses n-1 instead of n in\n",
    "the denominator to adjust for bias in sample variance estimation. \n",
    "\n",
    "This decision results in a so-called biased estimate of the variance. \n",
    "\n",
    "For large-scale language\n",
    "models (LLMs), where the embedding dimension n is significantly large, the\n",
    "difference between using n and n-1 is practically negligible. \n",
    "\n",
    "We chose this approach to ensure compatibility with the GPT-2 model's normalization layers and because it\n",
    "reflects TensorFlow's default behavior, which was used to implement the original GPT2 model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c30075c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
      "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n",
      "Mean:\n",
      " tensor([[-2.9802e-08],\n",
      "        [ 0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim = 5)\n",
    "out_ln  = ln(batch_example)\n",
    "print(out_ln)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81a5cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e01bb900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu, y_gelu_dist = gelu(x), relu(x), (gelu(x))/x\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu,y_gelu_dist], [\"GELU\", \"ReLU\", \"GELU_DIST\"]), 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c365cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),    \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ef2300c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768) #A\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8338b9",
   "metadata": {},
   "source": [
    "# SHORTCUT CONNECTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b697c61",
   "metadata": {},
   "source": [
    "![Screenshot](images/screenshot7.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f747b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e63ad",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The code implements a deep neural network with 5 layers, each consisting of a Linear\n",
    "layer and a GELU activation function. \n",
    "\n",
    "In the forward pass, we iteratively pass the input\n",
    "through the layers and optionally add the shortcut connections  if\n",
    "the self.use_shortcut attribute is set to True.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850696e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's use this code to first initialize a neural network without shortcut connections. Here,\n",
    "each layer will be initialized such that it accepts an example with 3 input values and returns\n",
    "3 output values. The last layer returns a single output value:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb4932ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # specify random seed for the initial weights for reproducibility\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02063c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63d59bd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In the preceding code, we specify a loss function that computes how close the model output\n",
    "and a user-specified target (here, for simplicity, the value 0) are. \n",
    "\n",
    "Then, when calling loss.backward(), PyTorch computes the loss gradient for each layer in the model. \n",
    "\n",
    "We can iterate through the weight parameters via model.named_parameters(). \n",
    "\n",
    "Suppose we have a 3×3 weight parameter matrix for a given layer. \n",
    "\n",
    "In that case, this layer will have 3×3 gradient values, and we print the mean absolute gradient of these 3×3 gradient values to\n",
    "obtain a single gradient value per layer to compare the gradients between layers more\n",
    "easily.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd40f4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In short, the .backward() method is a convenient method in PyTorch that computes loss\n",
    "gradients, which are required during model training, without implementing the math for the\n",
    "gradient calculation ourselves, thereby making working with deep neural networks much\n",
    "more accessible. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9b839",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now use the print_gradients function and apply it to the model without skip\n",
    "connections:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5cdca392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152039906941354\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed95d82",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see based on the output of the print_gradients function, the gradients become\n",
    "smaller as we progress from the last layer (layers.4) to the first layer (layers.0), which\n",
    "is a phenomenon called the vanishing gradient problem.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa92b4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's now instantiate a model with skip connections and see how it compares:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7194e3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a1472",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see, based on the output, the last layer (layers.4) still has a larger gradient\n",
    "than the other layers. \n",
    "\n",
    "However, the gradient value stabilizes as we progress towards the\n",
    "first layer (layers.0) and doesn't shrink to a vanishingly small value.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd87349",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In conclusion, shortcut connections are important for overcoming the limitations posed\n",
    "by the vanishing gradient problem in deep neural networks. \n",
    "\n",
    "Shortcut connections are a core building block of very large models such as LLMs, and they will help facilitate more effective\n",
    "training by ensuring consistent gradient flow across layers when we train the GPT model \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0355764",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 5: CODING ATTENTION AND LINEAR LAYERS IN A TRANSFORMER BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1cdc2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False)\n",
    "        norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),    \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aef78877",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransofrmerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadedAttention(\n",
    "            cfg[\"emb_dim\"], cfg[\"emb_dim\"], cfg[\"context_length\"],\n",
    "            cfg[\"drop_rate\"], cfg[\"n_heads\"], qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward (self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268da556",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Layer normalization (LayerNorm) is applied before each of these two components, and\n",
    "dropout is applied after them to regularize the model and prevent overfitting. \n",
    "\n",
    "This is also known as Pre-LayerNorm. \n",
    "\n",
    "Older architectures, such as the original transformer model,\n",
    "applied layer normalization after the self-attention and feed-forward networks instead,\n",
    "known as Post-LayerNorm, which often leads to worse training dynamics.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b3de627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransofrmerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db003c40",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see from the code output, the transformer block maintains the input dimensions\n",
    "in its output, indicating that the transformer architecture processes sequences of data\n",
    "without altering their shape throughout the network.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b11ebb9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "The preservation of shape throughout the transformer block architecture is not incidental\n",
    "but a crucial aspect of its design. \n",
    "\n",
    "This design enables its effective application across a wide\n",
    "range of sequence-to-sequence tasks, where each output vector directly corresponds to an\n",
    "input vector, maintaining a one-to-one relationship. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d69f799",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "However, the output is a context vector\n",
    "that encapsulates information from the entire input sequence.\n",
    "\n",
    "This means that while the physical dimensions of the sequence (length and feature size)\n",
    "remain unchanged as it passes through the transformer block, the content of each output\n",
    "vector is re-encoded to integrate contextual information from across the entire input\n",
    "sequence.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403551c2",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 6: ENTIRE GPT MODEL ARCHITECTURE IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941cd900",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The device setting will allow us to train the model on a CPU or GPU, depending on which device the input\n",
    "data sits\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b28e6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb  = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransofrmerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ef2fb85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76e164",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, the output tensor has the shape [2, 4, 50257], since we passed in 2 input\n",
    "texts with 4 tokens each. The last dimension, 50,257, corresponds to the vocabulary size of\n",
    "the tokenizer. In the next section, we will see how to convert each of these 50,257-\n",
    "dimensional output vectors back into tokens.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b134d41",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Using the numel() method, short for \"number of elements,\" we can collect the total\n",
    "number of parameters in the model's parameter tensors:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b25de605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011a00b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Earlier, we spoke of initializing a 124\n",
    "million parameter GPT model, so why is the actual number of parameters 163 million, as\n",
    "shown in the preceding code output?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3235865",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "The reason is a concept called weight tying that is used in the original GPT-2\n",
    "architecture, which means that the original GPT-2 architecture is reusing the weights from\n",
    "the token embedding layer in its output layer. \n",
    "\n",
    "To understand what this means, let's take a\n",
    "look at the shapes of the token embedding layer and linear output layer that we initialized\n",
    "on the model via the GPTModel earlier:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d7cfbf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed42f56",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see based on the print outputs, the weight tensors for both these layers have the\n",
    "same shape:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350bb954",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The token embedding and output layers are very large due to the number of rows for the\n",
    "50,257 in the tokenizer's vocabulary. Let's remove the output layer parameter count from\n",
    "the total GPT-2 model count according to the weight tying:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "38117b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2151f226",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, the model is now only 124 million parameters large, matching the original\n",
    "size of the GPT-2 model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7752dd4b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "Weight tying reduces the overall memory footprint and computational complexity of the\n",
    "model. However, in my experience, using separate token embedding and output layers\n",
    "results in better training and model performance; hence, we are using separate layers in\n",
    "our GPTModel implementation. The same is true for modern LLMs.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a3397b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Lastly, let us compute the memory requirements of the 163 million parameters in our\n",
    "GPTModel object:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4c8f883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4 #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e640d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "In conclusion, by calculating the memory requirements for the 163 million parameters in\n",
    "our GPTModel object and assuming each parameter is a 32-bit float taking up 4 bytes, we\n",
    "find that the total size of the model amounts to 621.83 MB, illustrating the relatively large\n",
    "storage capacity required to accommodate even relatively small LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f28e64",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "In this section, we implemented the GPTModel architecture and saw that it outputs\n",
    "numeric tensors of shape [batch_size, num_tokens, vocab_size]. In the next section,\n",
    "we will write the code to convert these output tensors into text.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf9a9d",
   "metadata": {},
   "source": [
    "Weight tying means sharing the same weight matrix between:\n",
    "\n",
    "The token embedding layer (converts token IDs to vectors), and\n",
    "\n",
    "The output projection layer (converts final vectors to logits over the vocabulary).\n",
    "\n",
    "So instead of having two separate matrices:\n",
    "\n",
    "E for input embedding: shape [vocab_size, emb_dim]\n",
    "\n",
    "W for output projection: shape [emb_dim, vocab_size]\n",
    "\n",
    "We reuse E.T as the output layer:\n",
    "\n",
    "logits = x @ E.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e34fb",
   "metadata": {},
   "source": [
    " Why Use Weight Tying?\n",
    "Fewer Parameters\n",
    "\n",
    "GPT-2 has a huge vocabulary (e.g., 50,000 tokens). Instead of storing two large matrices, weight tying saves memory.\n",
    "\n",
    "Improved Generalization\n",
    "\n",
    "Sharing weights forces the model to treat input and output tokens in a more consistent way.\n",
    "\n",
    "This helps especially in language modeling, where the same vocabulary is used at both ends.\n",
    "\n",
    "Empirical Gains\n",
    "\n",
    "Papers like \"Using the Output Embedding to Improve Language Models\" (Press & Wolf, 2017) showed that weight tying improves perplexity.\n",
    "\n",
    "🧠 Intuition\n",
    "Think of it like this:\n",
    "\n",
    "If the embedding for \"cat\" is a certain vector, then the output layer should also understand that same vector means \"cat\".\n",
    "\n",
    "Sharing weights enforces this symmetry: the model learns one set of weights that both understands and generates token representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f7341",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 7: GENERATING TEXT FROM OUTPUT TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e82bb33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens,context_size):\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :] # Get the last token's logits\n",
    "\n",
    "        probas = torch.softmax(logits, dim = -1)\n",
    "\n",
    "        idx_next = torch.argmax(probas, dim = -1, keepdim = True)\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim = 1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "963f52b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded context: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"Encoded context:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # Add batch dimension\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8fdc7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Next, we put the model into .eval() mode, which disables random components like\n",
    "dropout, which are only used during training, and use the generate_text_simple function\n",
    "on the encoded input tensor:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ce0d7710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,\n",
      "         49706, 43231, 47062, 34657]])\n",
      "Output length: 14\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(model, encoded_tensor, max_new_tokens = 10, context_size = GPT_CONFIG_124M[\"context_length\"]) \n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a746372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(out[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fb99ba8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307fcec0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, based on the preceding output, the model generated gibberish, which is not\n",
    "at all coherent text. \n",
    "\n",
    "What happened? \n",
    "\n",
    "The reason why the model is unable to produce coherent text is that we haven't trained it yet. \n",
    "\n",
    "So far, we just\n",
    "implemented the GPT architecture and initialized a GPT model instance with initial random\n",
    "weights.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f2a51bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Aeiman Byeswickattributeometer inspector Normandy freezerigrate\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # Add batch dimension\n",
    "    return encoded_tensor\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2bd9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "82fd45a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits,dim = -1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e4a8f192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[36397],\n",
      "         [39619],\n",
      "         [20610]],\n",
      "\n",
      "        [[ 8615],\n",
      "         [49289],\n",
      "         [47105]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b0ff7882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Gathering SerbianFriday\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "314a3e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([2.3466e-05, 2.0531e-05, 1.1733e-05])\n",
      "Text 2: tensor([4.2794e-05, 1.6248e-05, 1.1586e-05])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dfc58229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probabilities:\n",
      " tensor([-10.6600, -10.7936, -11.3531, -10.0591, -11.0276, -11.3658])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2), dim=0))\n",
    "print(\"Log probabilities:\\n\", log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6e5b46fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.8765)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11b4ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The goal is to make this average log probability as large as possible by optimizing the model weights.\n",
    "\n",
    "Due to the log, the largest possible value is 0, and we are currently far away from 0.\n",
    "\n",
    "In deep learning, instead of maximizing the average log-probability, it's a standard convention to minimize the negative average log-probability value; in our case, instead of maximizing -10.7722 so that it approaches 0, in deep learning, we would minimize 10.7722 so that it approaches 0.\n",
    "\n",
    "The value negative of -10.7722, i.e., 10.7722, is also called cross-entropy loss in deep learning.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "88827ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8765)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1c2de412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9276589",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Note that the targets are the token IDs, which also represent the index positions in the logits tensors that we want to maximize.\n",
    "    \n",
    "The cross_entropy function in PyTorch will automatically take care of applying the softmax and log-probability computation internally over those token indices in the logits that are to be maximized\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e6caf587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8765)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f308fef8",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abe6ba1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "A concept related to the cross-entropy loss is the perplexity of an LLM.\n",
    "\n",
    "The perplexity is simply the exponential of the cross-entropy loss.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a8e88a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(52918.7734)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7f52e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The perplexity is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that'd be 52918.7734 words or tokens).\n",
    "\n",
    "In other words, perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset.\n",
    "    \n",
    "Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5773d6",
   "metadata": {},
   "source": [
    "### Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a004480",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We use a relatively small dataset for training the LLM (in fact, only one short story)\n",
    "\n",
    "The reasons are:\n",
    "\n",
    "You can run the code examples in a few minutes on a laptop computer without a suitable GPU.\n",
    "\n",
    "The training finishes relatively fast (minutes instead of weeks), which is good for educational purposes.\n",
    "    \n",
    "We use a text from the public domain, which can be included in this GitHub repository without violating any usage rights or bloating the repository size.\n",
    "    \n",
    "For example, Llama 2 7B required 184,320 GPU hours on A100 GPUs to be trained on 2 trillion tokens\n",
    "\n",
    "At the time of this writing, the hourly cost of an 8xA100 cloud server at AWS is approximately 30 dollars. \n",
    "\n",
    "So, via an off-the-envelope calculation, training this LLM would cost 184,320 / 8 * 30 = 690,000 dollars\n",
    "\n",
    "Below, we use the same dataset we used in chapter 2.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0f787ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45e5e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = \"Harry Potter and The Half-Blood Prince.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "47e7d23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly nor'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "34897960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'his forehead.\\n\\n“I know he will.”\\n\\nThe scar had not pained Harry for nineteen years. All was well.\\n\\n'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[-99:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2f5ae4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "total_character = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fa7a8616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 6285449\n",
      "Tokens: 1669303\n"
     ]
    }
   ],
   "source": [
    "print(\"Characters:\", total_character)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8f2c4675",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size = 10, \n",
    "    max_length = GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_length\"] , \n",
    "    shuffle = True, \n",
    "    drop_last = True,\n",
    "    num_workers = 0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size = 10, \n",
    "    max_length = GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_length\"] , \n",
    "    shuffle = False, \n",
    "    drop_last = False,\n",
    "    num_workers = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "371070c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([10, 256]) torch.Size([10, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "Number of batches in train loader: 586\n",
      "Number of batches in validation loader: 66\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"Number of batches in train loader:\", len(train_loader))\n",
    "print(\"Number of batches in validation loader:\", len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9f98caf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 1500160\n",
      "Validation tokens: 167424\n",
      "All tokens: 1667584\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for x, y in train_loader:\n",
    "    train_tokens += x.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for x, y in val_loader:\n",
    "    val_tokens += x.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "52e924a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches = None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e506e2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "07a7f7f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m GPTModel(GPT_CONFIG_124M)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m calc_loss_loader(train_loader, model, device)\n\u001b[1;32m      5\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m calc_loss_loader(val_loader, model, device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss)\n",
      "Cell \u001b[0;32mIn[100], line 21\u001b[0m, in \u001b[0;36mcalc_loss_loader\u001b[0;34m(data_loader, model, device, num_batches)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m num_batches:\n\u001b[1;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m calc_loss_batch(input_batch, target_batch, model, device)\n\u001b[0;32m---> 21\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "42b9799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5c11a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a885f24",
   "metadata": {},
   "source": [
    "# TRAINING LOOP FOR THE LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f0e32363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, device, optimizer, num_epochs,\n",
    "    eval_freq, eval_iter, start_context,tokenizer):\n",
    "\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        \n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    \n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a6ef280c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.540, Val loss 10.585\n",
      "Ep 1 (Step 000010): Train loss 8.841, Val loss 8.952\n",
      "Ep 1 (Step 000020): Train loss 8.053, Val loss 8.153\n",
      "Ep 1 (Step 000030): Train loss 7.428, Val loss 7.548\n",
      "Ep 1 (Step 000040): Train loss 6.874, Val loss 7.198\n",
      "Ep 1 (Step 000050): Train loss 6.572, Val loss 6.994\n",
      "Ep 1 (Step 000060): Train loss 6.443, Val loss 6.871\n",
      "Ep 1 (Step 000070): Train loss 6.419, Val loss 6.760\n",
      "Ep 1 (Step 000080): Train loss 6.415, Val loss 6.622\n",
      "Ep 1 (Step 000090): Train loss 6.157, Val loss 6.503\n",
      "Ep 1 (Step 000100): Train loss 5.958, Val loss 6.455\n",
      "Ep 1 (Step 000110): Train loss 6.062, Val loss 6.411\n",
      "Ep 1 (Step 000120): Train loss 6.048, Val loss 6.395\n",
      "Ep 1 (Step 000130): Train loss 5.851, Val loss 6.313\n",
      "Ep 1 (Step 000140): Train loss 5.648, Val loss 6.235\n",
      "Ep 1 (Step 000150): Train loss 5.775, Val loss 6.222\n",
      "Ep 1 (Step 000160): Train loss 5.752, Val loss 6.147\n",
      "Ep 1 (Step 000170): Train loss 5.667, Val loss 6.121\n",
      "Ep 1 (Step 000180): Train loss 5.866, Val loss 6.115\n",
      "Ep 1 (Step 000190): Train loss 5.491, Val loss 6.077\n",
      "Ep 1 (Step 000200): Train loss 5.581, Val loss 6.051\n",
      "Ep 1 (Step 000210): Train loss 5.612, Val loss 6.035\n",
      "Ep 1 (Step 000220): Train loss 5.683, Val loss 6.004\n",
      "Ep 1 (Step 000230): Train loss 5.640, Val loss 5.962\n",
      "Ep 1 (Step 000240): Train loss 5.434, Val loss 5.929\n",
      "Ep 1 (Step 000250): Train loss 5.623, Val loss 5.912\n",
      "Ep 1 (Step 000260): Train loss 5.519, Val loss 5.910\n",
      "Ep 1 (Step 000270): Train loss 5.387, Val loss 5.892\n",
      "Ep 1 (Step 000280): Train loss 5.298, Val loss 5.873\n",
      "Harry potter was stunned the Prime Minister, and the Prime Minister, and the Prime Minister.  \"I think you, and the Prime Minister, and the Prime Minister, and the Prime Minister, and the Prime Minister, and the Prime Minister, and the Prime Minister\n",
      "Ep 2 (Step 000290): Train loss 5.274, Val loss 5.861\n",
      "Ep 2 (Step 000300): Train loss 5.222, Val loss 5.833\n",
      "Ep 2 (Step 000310): Train loss 5.268, Val loss 5.825\n",
      "Ep 2 (Step 000320): Train loss 5.241, Val loss 5.825\n",
      "Ep 2 (Step 000330): Train loss 5.140, Val loss 5.803\n",
      "Ep 2 (Step 000340): Train loss 5.176, Val loss 5.766\n",
      "Ep 2 (Step 000350): Train loss 5.236, Val loss 5.776\n",
      "Ep 2 (Step 000360): Train loss 5.092, Val loss 5.732\n",
      "Ep 2 (Step 000370): Train loss 5.200, Val loss 5.722\n",
      "Ep 2 (Step 000380): Train loss 4.933, Val loss 5.727\n",
      "Ep 2 (Step 000390): Train loss 5.053, Val loss 5.696\n",
      "Ep 2 (Step 000400): Train loss 5.033, Val loss 5.686\n",
      "Ep 2 (Step 000410): Train loss 4.907, Val loss 5.673\n",
      "Ep 2 (Step 000420): Train loss 4.982, Val loss 5.687\n",
      "Ep 2 (Step 000430): Train loss 5.077, Val loss 5.691\n",
      "Ep 2 (Step 000440): Train loss 5.010, Val loss 5.659\n",
      "Ep 2 (Step 000450): Train loss 4.973, Val loss 5.614\n",
      "Ep 2 (Step 000460): Train loss 4.957, Val loss 5.625\n",
      "Ep 2 (Step 000470): Train loss 4.948, Val loss 5.603\n",
      "Ep 2 (Step 000480): Train loss 5.023, Val loss 5.573\n",
      "Ep 2 (Step 000490): Train loss 4.942, Val loss 5.573\n",
      "Ep 2 (Step 000500): Train loss 4.804, Val loss 5.581\n",
      "Ep 2 (Step 000510): Train loss 4.842, Val loss 5.581\n",
      "Ep 2 (Step 000520): Train loss 4.724, Val loss 5.560\n",
      "Ep 2 (Step 000530): Train loss 4.578, Val loss 5.561\n",
      "Ep 2 (Step 000540): Train loss 4.725, Val loss 5.525\n",
      "Ep 2 (Step 000550): Train loss 4.733, Val loss 5.505\n",
      "Ep 2 (Step 000560): Train loss 4.823, Val loss 5.511\n",
      "Ep 2 (Step 000570): Train loss 4.726, Val loss 5.515\n",
      "Harry potter was stunned.  \"No,\" said Harry, I am sure that you have been able to the Dark Lord Voldemort's not to the door.  \"I'm not have been a few of the Dark Lord Voldemort's not to the door. \n",
      "Ep 3 (Step 000580): Train loss 4.912, Val loss 5.507\n",
      "Ep 3 (Step 000590): Train loss 4.705, Val loss 5.504\n",
      "Ep 3 (Step 000600): Train loss 4.652, Val loss 5.481\n",
      "Ep 3 (Step 000610): Train loss 4.521, Val loss 5.488\n",
      "Ep 3 (Step 000620): Train loss 4.617, Val loss 5.489\n",
      "Ep 3 (Step 000630): Train loss 4.677, Val loss 5.499\n",
      "Ep 3 (Step 000640): Train loss 4.678, Val loss 5.493\n",
      "Ep 3 (Step 000650): Train loss 4.689, Val loss 5.475\n",
      "Ep 3 (Step 000660): Train loss 4.586, Val loss 5.466\n",
      "Ep 3 (Step 000670): Train loss 4.519, Val loss 5.455\n",
      "Ep 3 (Step 000680): Train loss 4.493, Val loss 5.462\n",
      "Ep 3 (Step 000690): Train loss 4.607, Val loss 5.452\n",
      "Ep 3 (Step 000700): Train loss 4.522, Val loss 5.443\n",
      "Ep 3 (Step 000710): Train loss 4.621, Val loss 5.431\n",
      "Ep 3 (Step 000720): Train loss 4.554, Val loss 5.419\n",
      "Ep 3 (Step 000730): Train loss 4.455, Val loss 5.428\n",
      "Ep 3 (Step 000740): Train loss 4.338, Val loss 5.422\n",
      "Ep 3 (Step 000750): Train loss 4.466, Val loss 5.425\n",
      "Ep 3 (Step 000760): Train loss 4.453, Val loss 5.401\n",
      "Ep 3 (Step 000770): Train loss 4.449, Val loss 5.384\n",
      "Ep 3 (Step 000780): Train loss 4.333, Val loss 5.387\n",
      "Ep 3 (Step 000790): Train loss 4.475, Val loss 5.377\n",
      "Ep 3 (Step 000800): Train loss 4.460, Val loss 5.376\n",
      "Ep 3 (Step 000810): Train loss 4.534, Val loss 5.370\n",
      "Ep 3 (Step 000820): Train loss 4.409, Val loss 5.371\n",
      "Ep 3 (Step 000830): Train loss 4.366, Val loss 5.372\n",
      "Ep 3 (Step 000840): Train loss 4.509, Val loss 5.342\n",
      "Ep 3 (Step 000850): Train loss 4.432, Val loss 5.358\n",
      "Ep 3 (Step 000860): Train loss 4.391, Val loss 5.346\n",
      "Harry potter was stunned. \"I am,\" said Harry. \"I'm going to do you, I've got a moment, I am sure you, I've got a moment, I am not to be a little, I've got to be a little-\n",
      "Ep 4 (Step 000870): Train loss 4.219, Val loss 5.353\n",
      "Ep 4 (Step 000880): Train loss 4.313, Val loss 5.346\n",
      "Ep 4 (Step 000890): Train loss 4.364, Val loss 5.342\n",
      "Ep 4 (Step 000900): Train loss 4.339, Val loss 5.342\n",
      "Ep 4 (Step 000910): Train loss 4.272, Val loss 5.351\n",
      "Ep 4 (Step 000920): Train loss 4.148, Val loss 5.341\n",
      "Ep 4 (Step 000930): Train loss 4.262, Val loss 5.350\n",
      "Ep 4 (Step 000940): Train loss 4.123, Val loss 5.353\n",
      "Ep 4 (Step 000950): Train loss 4.200, Val loss 5.356\n",
      "Ep 4 (Step 000960): Train loss 4.177, Val loss 5.329\n",
      "Ep 4 (Step 000970): Train loss 4.025, Val loss 5.337\n",
      "Ep 4 (Step 000980): Train loss 4.062, Val loss 5.320\n",
      "Ep 4 (Step 000990): Train loss 4.094, Val loss 5.334\n",
      "Ep 4 (Step 001000): Train loss 4.274, Val loss 5.323\n",
      "Ep 4 (Step 001010): Train loss 4.116, Val loss 5.330\n",
      "Ep 4 (Step 001020): Train loss 4.194, Val loss 5.306\n",
      "Ep 4 (Step 001030): Train loss 4.100, Val loss 5.296\n",
      "Ep 4 (Step 001040): Train loss 4.149, Val loss 5.287\n",
      "Ep 4 (Step 001050): Train loss 4.117, Val loss 5.295\n",
      "Ep 4 (Step 001060): Train loss 4.080, Val loss 5.307\n",
      "Ep 4 (Step 001070): Train loss 4.010, Val loss 5.318\n",
      "Ep 4 (Step 001080): Train loss 3.948, Val loss 5.297\n",
      "Ep 4 (Step 001090): Train loss 4.016, Val loss 5.314\n",
      "Ep 4 (Step 001100): Train loss 4.061, Val loss 5.296\n",
      "Ep 4 (Step 001110): Train loss 3.962, Val loss 5.295\n",
      "Ep 4 (Step 001120): Train loss 3.996, Val loss 5.303\n",
      "Ep 4 (Step 001130): Train loss 3.876, Val loss 5.288\n",
      "Ep 4 (Step 001140): Train loss 3.909, Val loss 5.272\n",
      "Ep 4 (Step 001150): Train loss 3.940, Val loss 5.278\n",
      "Harry potter was stunned, and the room, and the room, and the room was a little bottle of the room.  \"I'm going to the room,\" said Dumbledore, \"I'm going to the other hand, \"I'm not want to the other\n",
      "Ep 5 (Step 001160): Train loss 4.025, Val loss 5.292\n",
      "Ep 5 (Step 001170): Train loss 3.847, Val loss 5.277\n",
      "Ep 5 (Step 001180): Train loss 3.912, Val loss 5.278\n",
      "Ep 5 (Step 001190): Train loss 3.962, Val loss 5.271\n",
      "Ep 5 (Step 001200): Train loss 3.888, Val loss 5.272\n",
      "Ep 5 (Step 001210): Train loss 3.825, Val loss 5.280\n",
      "Ep 5 (Step 001220): Train loss 3.934, Val loss 5.280\n",
      "Ep 5 (Step 001230): Train loss 3.917, Val loss 5.262\n",
      "Ep 5 (Step 001240): Train loss 3.906, Val loss 5.266\n",
      "Ep 5 (Step 001250): Train loss 3.852, Val loss 5.275\n",
      "Ep 5 (Step 001260): Train loss 3.733, Val loss 5.293\n",
      "Ep 5 (Step 001270): Train loss 3.806, Val loss 5.276\n",
      "Ep 5 (Step 001280): Train loss 3.804, Val loss 5.290\n",
      "Ep 5 (Step 001290): Train loss 3.877, Val loss 5.280\n",
      "Ep 5 (Step 001300): Train loss 3.718, Val loss 5.263\n",
      "Ep 5 (Step 001310): Train loss 3.740, Val loss 5.279\n",
      "Ep 5 (Step 001320): Train loss 3.635, Val loss 5.285\n",
      "Ep 5 (Step 001330): Train loss 3.784, Val loss 5.283\n",
      "Ep 5 (Step 001340): Train loss 3.705, Val loss 5.270\n",
      "Ep 5 (Step 001350): Train loss 3.723, Val loss 5.263\n",
      "Ep 5 (Step 001360): Train loss 3.681, Val loss 5.259\n",
      "Ep 5 (Step 001370): Train loss 3.685, Val loss 5.266\n",
      "Ep 5 (Step 001380): Train loss 3.691, Val loss 5.289\n",
      "Ep 5 (Step 001390): Train loss 3.619, Val loss 5.277\n",
      "Ep 5 (Step 001400): Train loss 3.658, Val loss 5.265\n",
      "Ep 5 (Step 001410): Train loss 3.657, Val loss 5.269\n",
      "Ep 5 (Step 001420): Train loss 3.525, Val loss 5.272\n",
      "Ep 5 (Step 001430): Train loss 3.646, Val loss 5.265\n",
      "Ep 5 (Step 001440): Train loss 3.581, Val loss 5.261\n",
      "Harry potter was stunned to be able to be a few of the room.     \"I don't know what?\" said Harry, as I don't know what I don't have to be a few of course, but I'm going to be a\n",
      "Ep 6 (Step 001450): Train loss 3.555, Val loss 5.251\n",
      "Ep 6 (Step 001460): Train loss 3.545, Val loss 5.246\n",
      "Ep 6 (Step 001470): Train loss 3.625, Val loss 5.257\n",
      "Ep 6 (Step 001480): Train loss 3.558, Val loss 5.278\n",
      "Ep 6 (Step 001490): Train loss 3.517, Val loss 5.276\n",
      "Ep 6 (Step 001500): Train loss 3.431, Val loss 5.306\n",
      "Ep 6 (Step 001510): Train loss 3.310, Val loss 5.291\n",
      "Ep 6 (Step 001520): Train loss 3.424, Val loss 5.303\n",
      "Ep 6 (Step 001530): Train loss 3.554, Val loss 5.284\n",
      "Ep 6 (Step 001540): Train loss 3.587, Val loss 5.300\n",
      "Ep 6 (Step 001550): Train loss 3.406, Val loss 5.312\n",
      "Ep 6 (Step 001560): Train loss 3.471, Val loss 5.324\n",
      "Ep 6 (Step 001570): Train loss 3.422, Val loss 5.294\n",
      "Ep 6 (Step 001580): Train loss 3.504, Val loss 5.291\n",
      "Ep 6 (Step 001590): Train loss 3.347, Val loss 5.289\n",
      "Ep 6 (Step 001600): Train loss 3.377, Val loss 5.288\n",
      "Ep 6 (Step 001610): Train loss 3.434, Val loss 5.310\n",
      "Ep 6 (Step 001620): Train loss 3.310, Val loss 5.304\n",
      "Ep 6 (Step 001630): Train loss 3.270, Val loss 5.289\n",
      "Ep 6 (Step 001640): Train loss 3.342, Val loss 5.291\n",
      "Ep 6 (Step 001650): Train loss 3.325, Val loss 5.292\n",
      "Ep 6 (Step 001660): Train loss 3.381, Val loss 5.279\n",
      "Ep 6 (Step 001670): Train loss 3.208, Val loss 5.288\n",
      "Ep 6 (Step 001680): Train loss 3.228, Val loss 5.288\n",
      "Ep 6 (Step 001690): Train loss 3.426, Val loss 5.279\n",
      "Ep 6 (Step 001700): Train loss 3.273, Val loss 5.273\n",
      "Ep 6 (Step 001710): Train loss 3.168, Val loss 5.281\n",
      "Ep 6 (Step 001720): Train loss 3.233, Val loss 5.293\n",
      "Ep 6 (Step 001730): Train loss 3.204, Val loss 5.295\n",
      "Harry potter was stunned; he could see the door. \"I'm sorry, I'm sure, I'm not know, I'm sure, I'm not have been a moment, I'm sure, I'm sure, I'm sure, I'm not to\n",
      "Ep 7 (Step 001740): Train loss 3.194, Val loss 5.297\n",
      "Ep 7 (Step 001750): Train loss 3.103, Val loss 5.300\n",
      "Ep 7 (Step 001760): Train loss 3.057, Val loss 5.315\n",
      "Ep 7 (Step 001770): Train loss 3.135, Val loss 5.327\n",
      "Ep 7 (Step 001780): Train loss 3.050, Val loss 5.340\n",
      "Ep 7 (Step 001790): Train loss 2.985, Val loss 5.328\n",
      "Ep 7 (Step 001800): Train loss 3.063, Val loss 5.337\n",
      "Ep 7 (Step 001810): Train loss 3.041, Val loss 5.312\n",
      "Ep 7 (Step 001820): Train loss 3.079, Val loss 5.331\n",
      "Ep 7 (Step 001830): Train loss 3.035, Val loss 5.347\n",
      "Ep 7 (Step 001840): Train loss 2.970, Val loss 5.339\n",
      "Ep 7 (Step 001850): Train loss 3.014, Val loss 5.348\n",
      "Ep 7 (Step 001860): Train loss 3.040, Val loss 5.348\n",
      "Ep 7 (Step 001870): Train loss 3.035, Val loss 5.359\n",
      "Ep 7 (Step 001880): Train loss 2.863, Val loss 5.356\n",
      "Ep 7 (Step 001890): Train loss 2.976, Val loss 5.351\n",
      "Ep 7 (Step 001900): Train loss 3.000, Val loss 5.358\n",
      "Ep 7 (Step 001910): Train loss 2.905, Val loss 5.361\n",
      "Ep 7 (Step 001920): Train loss 2.838, Val loss 5.336\n",
      "Ep 7 (Step 001930): Train loss 2.911, Val loss 5.360\n",
      "Ep 7 (Step 001940): Train loss 2.777, Val loss 5.354\n",
      "Ep 7 (Step 001950): Train loss 2.792, Val loss 5.379\n",
      "Ep 7 (Step 001960): Train loss 2.897, Val loss 5.373\n",
      "Ep 7 (Step 001970): Train loss 2.820, Val loss 5.382\n",
      "Ep 7 (Step 001980): Train loss 2.832, Val loss 5.368\n",
      "Ep 7 (Step 001990): Train loss 2.743, Val loss 5.353\n",
      "Ep 7 (Step 002000): Train loss 2.853, Val loss 5.371\n",
      "Ep 7 (Step 002010): Train loss 2.676, Val loss 5.350\n",
      "Ep 7 (Step 002020): Train loss 2.732, Val loss 5.353\n",
      "Harry potter was stunned; he could be a very long time to see Dumbledore's been able to be able to be an hour. He was a very long-Making.  \"I don't have?\" said Harry, but he had been able to be in the\n",
      "Ep 8 (Step 002030): Train loss 2.644, Val loss 5.404\n",
      "Ep 8 (Step 002040): Train loss 2.750, Val loss 5.403\n",
      "Ep 8 (Step 002050): Train loss 2.649, Val loss 5.422\n",
      "Ep 8 (Step 002060): Train loss 2.678, Val loss 5.429\n",
      "Ep 8 (Step 002070): Train loss 2.619, Val loss 5.416\n",
      "Ep 8 (Step 002080): Train loss 2.640, Val loss 5.424\n",
      "Ep 8 (Step 002090): Train loss 2.639, Val loss 5.450\n",
      "Ep 8 (Step 002100): Train loss 2.588, Val loss 5.452\n",
      "Ep 8 (Step 002110): Train loss 2.525, Val loss 5.450\n",
      "Ep 8 (Step 002120): Train loss 2.645, Val loss 5.474\n",
      "Ep 8 (Step 002130): Train loss 2.473, Val loss 5.470\n",
      "Ep 8 (Step 002140): Train loss 2.607, Val loss 5.470\n",
      "Ep 8 (Step 002150): Train loss 2.475, Val loss 5.483\n",
      "Ep 8 (Step 002160): Train loss 2.459, Val loss 5.486\n",
      "Ep 8 (Step 002170): Train loss 2.384, Val loss 5.481\n",
      "Ep 8 (Step 002180): Train loss 2.466, Val loss 5.498\n",
      "Ep 8 (Step 002190): Train loss 2.466, Val loss 5.501\n",
      "Ep 8 (Step 002200): Train loss 2.408, Val loss 5.522\n",
      "Ep 8 (Step 002210): Train loss 2.461, Val loss 5.521\n",
      "Ep 8 (Step 002220): Train loss 2.423, Val loss 5.518\n",
      "Ep 8 (Step 002230): Train loss 2.450, Val loss 5.496\n",
      "Ep 8 (Step 002240): Train loss 2.413, Val loss 5.502\n",
      "Ep 8 (Step 002250): Train loss 2.395, Val loss 5.498\n",
      "Ep 8 (Step 002260): Train loss 2.383, Val loss 5.502\n",
      "Ep 8 (Step 002270): Train loss 2.397, Val loss 5.506\n",
      "Ep 8 (Step 002280): Train loss 2.288, Val loss 5.500\n",
      "Ep 8 (Step 002290): Train loss 2.317, Val loss 5.502\n",
      "Ep 8 (Step 002300): Train loss 2.321, Val loss 5.482\n",
      "Ep 8 (Step 002310): Train loss 2.221, Val loss 5.479\n",
      "Harry potter was stunned; he could reach him into the castle to see what looked like that was as though Harry's face.     \"I don't know what I don't know what's got a few of my dear boy, I'm sorry,\n",
      "Ep 9 (Step 002320): Train loss 2.209, Val loss 5.515\n",
      "Ep 9 (Step 002330): Train loss 2.300, Val loss 5.538\n",
      "Ep 9 (Step 002340): Train loss 2.296, Val loss 5.549\n",
      "Ep 9 (Step 002350): Train loss 2.150, Val loss 5.569\n",
      "Ep 9 (Step 002360): Train loss 2.174, Val loss 5.590\n",
      "Ep 9 (Step 002370): Train loss 2.173, Val loss 5.606\n",
      "Ep 9 (Step 002380): Train loss 2.135, Val loss 5.623\n",
      "Ep 9 (Step 002390): Train loss 2.013, Val loss 5.615\n",
      "Ep 9 (Step 002400): Train loss 2.144, Val loss 5.626\n",
      "Ep 9 (Step 002410): Train loss 2.137, Val loss 5.617\n",
      "Ep 9 (Step 002420): Train loss 2.144, Val loss 5.651\n",
      "Ep 9 (Step 002430): Train loss 2.009, Val loss 5.622\n",
      "Ep 9 (Step 002440): Train loss 2.119, Val loss 5.616\n",
      "Ep 9 (Step 002450): Train loss 1.973, Val loss 5.651\n",
      "Ep 9 (Step 002460): Train loss 2.061, Val loss 5.660\n",
      "Ep 9 (Step 002470): Train loss 2.012, Val loss 5.672\n",
      "Ep 9 (Step 002480): Train loss 1.924, Val loss 5.650\n",
      "Ep 9 (Step 002490): Train loss 1.959, Val loss 5.673\n",
      "Ep 9 (Step 002500): Train loss 1.912, Val loss 5.665\n",
      "Ep 9 (Step 002510): Train loss 1.927, Val loss 5.672\n",
      "Ep 9 (Step 002520): Train loss 1.860, Val loss 5.646\n",
      "Ep 9 (Step 002530): Train loss 1.853, Val loss 5.671\n",
      "Ep 9 (Step 002540): Train loss 1.814, Val loss 5.672\n",
      "Ep 9 (Step 002550): Train loss 1.894, Val loss 5.656\n",
      "Ep 9 (Step 002560): Train loss 1.868, Val loss 5.674\n",
      "Ep 9 (Step 002570): Train loss 1.839, Val loss 5.676\n",
      "Ep 9 (Step 002580): Train loss 1.830, Val loss 5.678\n",
      "Ep 9 (Step 002590): Train loss 1.824, Val loss 5.678\n",
      "Ep 9 (Step 002600): Train loss 1.825, Val loss 5.680\n",
      "Harry potter was stunned of his head.  \"I am sure that Dumbledore has been a very long time.  \"I thought you are you are, sir,\" said Dumbledore, \"I have been in my office at the Ministry.\"  He was as\n",
      "Ep 10 (Step 002610): Train loss 1.810, Val loss 5.713\n",
      "Ep 10 (Step 002620): Train loss 1.802, Val loss 5.740\n",
      "Ep 10 (Step 002630): Train loss 1.700, Val loss 5.774\n",
      "Ep 10 (Step 002640): Train loss 1.612, Val loss 5.792\n",
      "Ep 10 (Step 002650): Train loss 1.673, Val loss 5.785\n",
      "Ep 10 (Step 002660): Train loss 1.685, Val loss 5.787\n",
      "Ep 10 (Step 002670): Train loss 1.664, Val loss 5.795\n",
      "Ep 10 (Step 002680): Train loss 1.716, Val loss 5.791\n",
      "Ep 10 (Step 002690): Train loss 1.705, Val loss 5.825\n",
      "Ep 10 (Step 002700): Train loss 1.613, Val loss 5.847\n",
      "Ep 10 (Step 002710): Train loss 1.625, Val loss 5.809\n",
      "Ep 10 (Step 002720): Train loss 1.625, Val loss 5.843\n",
      "Ep 10 (Step 002730): Train loss 1.693, Val loss 5.835\n",
      "Ep 10 (Step 002740): Train loss 1.553, Val loss 5.874\n",
      "Ep 10 (Step 002750): Train loss 1.591, Val loss 5.819\n",
      "Ep 10 (Step 002760): Train loss 1.629, Val loss 5.859\n",
      "Ep 10 (Step 002770): Train loss 1.596, Val loss 5.874\n",
      "Ep 10 (Step 002780): Train loss 1.567, Val loss 5.848\n",
      "Ep 10 (Step 002790): Train loss 1.599, Val loss 5.868\n",
      "Ep 10 (Step 002800): Train loss 1.484, Val loss 5.868\n",
      "Ep 10 (Step 002810): Train loss 1.502, Val loss 5.857\n",
      "Ep 10 (Step 002820): Train loss 1.463, Val loss 5.885\n",
      "Ep 10 (Step 002830): Train loss 1.487, Val loss 5.858\n",
      "Ep 10 (Step 002840): Train loss 1.466, Val loss 5.863\n",
      "Ep 10 (Step 002850): Train loss 1.481, Val loss 5.879\n",
      "Ep 10 (Step 002860): Train loss 1.463, Val loss 5.877\n",
      "Ep 10 (Step 002870): Train loss 1.448, Val loss 5.850\n",
      "Ep 10 (Step 002880): Train loss 1.455, Val loss 5.888\n",
      "Harry potter was stunned into the room of a large and watery drizzle, and Hermi-one could not suppress the open. Harry's face, and saw the slightest, then had been seen in his hand.  \"So how are you see, I\n",
      "Training completed in 39.77 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "train_losses, val_losses, track_tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, device, num_epochs = num_epochs,\n",
    "    optimizer = optimizer, eval_freq = 10, eval_iter = 10,\n",
    "    start_context = \"Harry potter was stunned\", tokenizer = tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "06decc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Perplexity: 4.283, Val Perplexity: 360.753\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "train_perplexity = math.exp(train_losses[-1])\n",
    "val_perplexity = math.exp(val_losses[-1])\n",
    "\n",
    "print(f\"Train Perplexity: {train_perplexity:.3f}, Val Perplexity: {val_perplexity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e957b1b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss-plot.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 25\u001b[0m epochs_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, num_epochs, \u001b[38;5;28mlen\u001b[39m(train_losses))\n\u001b[1;32m     26\u001b[0m plot_losses(epochs_tensor, track_tokens_seen, train_losses, val_losses)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, track_tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea496dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
