{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4883d51",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d3b9ae30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The verdict\n",
      "Edith wharton\n",
      "\n",
      "I had always thought Jack Gisburn rather a cheap genius--though a good f\n",
      "total number of characters are : 20415\n"
     ]
    }
   ],
   "source": [
    "with open(\"wharton_verdict.txt\",\"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(raw_text[:99])\n",
    "print(f'total number of characters are : {len(raw_text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca8296",
   "metadata": {},
   "source": [
    "Our goal is to tokenize this 20,479-character short story into individual words and special\n",
    "characters that we can then turn into embeddings for LLM training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "439231ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello,', ' ', 'world.', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'text', ' ', 'file!!!.', ' ', 'It', ' ', 'contains', ' ', 'some', ' ', 'text.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"hello, world. This is a text file!!!. It contains some text.\"\n",
    "result = re.split(r'(\\s)', text) ## \\s splits wherever thiere is a whitespace\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1868f25",
   "metadata": {},
   "source": [
    "The result is a list of individual words, whitespaces, and punctuation characters\n",
    "\n",
    "\n",
    "Right now the commas and fullstops are part of the words as they dont have white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a94c0b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'text', ' ', 'file!!!', '.', '', ' ', 'It', ' ', 'contains', ' ', 'some', ' ', 'text', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a3341",
   "metadata": {},
   "source": [
    "We can see that the words and punctuation characters are now separate list entries just as\n",
    "we wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad772734",
   "metadata": {},
   "source": [
    "A small remaining issue is that the list still includes whitespace characters. Optionally, we\n",
    "can remove these redundant characters safely as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88b9fcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '.', 'This', 'is', 'a', 'text', 'file!!!', '.', 'It', 'contains', 'some', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f29c2d",
   "metadata": {},
   "source": [
    "\n",
    "REMOVING WHITESPACES OR NOT\n",
    "\n",
    "\n",
    "When developing a simple tokenizer, whether we should encode whitespaces as\n",
    "separate characters or just remove them depends on our application and its\n",
    "requirements. Removing whitespaces reduces the memory and computing\n",
    "requirements. However, keeping whitespaces can be useful if we train models that\n",
    "are sensitive to the exact structure of the text (for example, Python code, which is\n",
    "sensitive to indentation and spacing). Here, we remove whitespaces for simplicity\n",
    "and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme\n",
    "that includes whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01e65ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '.', 'This', 'is', 'a', 'text', 'file', '!', '!', '!', '.', 'It', 'contains', 'some', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.:;\"!?()\\'_]|--|\\s)', text)\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d317c6",
   "metadata": {},
   "source": [
    "Now we apply this basic tokenizer to edith whartons short story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a3c24e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_tokenizer(text):\n",
    "    result = re.split(r'([,.:;\"!?()\\'_]|--|\\s)', text)\n",
    "    result = [item for item in result if item.strip()]\n",
    "    print(result[:50])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "14c68624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'verdict', 'Edith', 'wharton', 'I', 'had', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow']\n"
     ]
    }
   ],
   "source": [
    "pre_processed = basic_tokenizer(raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf0bb2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total numeber of words : 4650\n"
     ]
    }
   ],
   "source": [
    "print(f'the total numeber of words : {len(pre_processed)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61627c",
   "metadata": {},
   "source": [
    "## Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f0687d",
   "metadata": {},
   "source": [
    "In the previous section, we tokenized Edith Wharton's short story and assigned it to a\n",
    "Python variable called preprocessed. Let's now create a list of all unique tokens and sort\n",
    "them alphabetically to determine the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "65475649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'AM', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At']\n",
      "the total number of unique words : 1150\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(pre_processed))\n",
    "vocab_size = len(all_words)\n",
    "print(all_words[:20])\n",
    "print(f'the total number of unique words : {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "50f0b28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n",
      "1006\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "print(vocab['and'])\n",
    "print(vocab['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd41276",
   "metadata": {},
   "source": [
    "As we can see, based on the output above, the dictionary contains individual tokens\n",
    "associated with unique integer labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5642d",
   "metadata": {},
   "source": [
    "Later in this book, when we want to convert the outputs of an LLM from numbers back into\n",
    "text, we also need a way to turn token IDs into text. \n",
    "\n",
    "For this, we can create an inverse\n",
    "version of the vocabulary that maps token IDs back to corresponding text tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8794ca76",
   "metadata": {},
   "source": [
    "Let's implement a complete tokenizer class in Python.\n",
    "\n",
    "The class will have an encode method that splits\n",
    "text into tokens and carries out the string-to-integer mapping to produce token IDs via the\n",
    "vocabulary. \n",
    "\n",
    "In addition, we implement a decode method that carries out the reverse\n",
    "integer-to-string mapping to convert the token IDs back into text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37417a52",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
    "    \n",
    "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
    "\n",
    "Step 3: Process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back into text\n",
    "\n",
    "Step 5: Replace spaces before the specified punctuation\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ecf47953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int =vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        pre_processed = re.split(r'([,.:;\"!?()\\'_]|--|\\s)', text)   \n",
    "        pre_processed = [item for item in pre_processed if item.strip()]\n",
    "        ids = [self.str_to_int[word] for word in pre_processed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])   \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)#remove unnecessary spaces before punctuation marks in the string text,   r'\\1'This tells Python to replace the entire match (space + punctuation) with just the punctuation mark (i.e., what's in group 1), removing the space.\n",
    "        return text\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1078ffd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 2, 868, 1006, 616, 547, 764, 5, 1146, 611, 5, 1, 74, 7, 41, 869, 1128, 772, 812, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cdf2d906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0e6b2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing\n",
    "text based on a snippet from the training set. \n",
    "\n",
    "Let's now apply it to a new text sample that\n",
    "is not contained in the training set:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "203abb43",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello, world. This is a text file!!!. It contains some text.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n",
      "Cell \u001b[0;32mIn[68], line 9\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m pre_processed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!?()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m_]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)   \n\u001b[1;32m      8\u001b[0m pre_processed \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m pre_processed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m pre_processed]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'hello'"
     ]
    }
   ],
   "source": [
    "text = \"hello, world. This is a text file!!!. It contains some text.\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8fffc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "The problem is that the word \"Hello\" was not used in the The Verdict short story. \n",
    "\n",
    "Hence, it\n",
    "is not contained in the vocabulary. \n",
    "\n",
    "This highlights the need to consider large and diverse\n",
    "training sets to extend the vocabulary when working on LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640cf03",
   "metadata": {},
   "source": [
    "In the previous section, we implemented a simple tokenizer and applied it to a passage\n",
    "from the training set. \n",
    "\n",
    "In this section, we will modify this tokenizer to handle unknown\n",
    "words.\n",
    "\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
    "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
    "<|endoftext|>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e062a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(pre_processed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be9069f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fd7485e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1147)\n",
      "('your', 1148)\n",
      "('yourself', 1149)\n",
      "('<|endoftext|>', 1150)\n",
      "('<|unk|>', 1151)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "585f6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        pre_processed = re.split(r'([,.:;\"!?()\\'_]|--|\\s)', text)   \n",
    "        pre_processed = [item for item in pre_processed if item.strip()]\n",
    "        pre_processed = [item if item in self.str_to_int else \"<|unk|>\" for item in pre_processed]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in pre_processed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "acb46d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, do you like tea and cars?<|endoftext|> in the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"hello, do you like tea and cars?\"\n",
    "text2 = \"in the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \"<|endoftext|> \".join([text1, text2])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed784ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1151,\n",
       " 5,\n",
       " 369,\n",
       " 1146,\n",
       " 643,\n",
       " 993,\n",
       " 169,\n",
       " 1151,\n",
       " 10,\n",
       " 1150,\n",
       " 583,\n",
       " 1006,\n",
       " 974,\n",
       " 1002,\n",
       " 739,\n",
       " 1006,\n",
       " 1151,\n",
       " 7]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "07ff3e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea and <|unk|>? <|endoftext|> in the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef267ff",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "So far, we have discussed tokenization as an essential step in processing text as input to\n",
    "LLMs. Depending on the LLM, some researchers also consider additional special tokens such\n",
    "as the following:\n",
    "\n",
    "[BOS] (beginning of sequence): This token marks the start of a text. It\n",
    "signifies to the LLM where a piece of content begins.\n",
    "\n",
    "[EOS] (end of sequence): This token is positioned at the end of a text,\n",
    "and is especially useful when concatenating multiple unrelated texts,\n",
    "similar to <|endoftext|>. For instance, when combining two different\n",
    "Wikipedia articles or books, the [EOS] token indicates where one article\n",
    "ends and the next one begins.\n",
    "\n",
    "[PAD] (padding): When training LLMs with batch sizes larger than one,\n",
    "the batch might contain texts of varying lengths. To ensure all texts have\n",
    "the same length, the shorter texts are extended or \"padded\" using the\n",
    "[PAD] token, up to the length of the longest text in the batch.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c172e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that the tokenizer used for GPT models does not need any of these tokens mentioned\n",
    "above but only uses an <|endoftext|> token for simplicity\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d3eca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "the tokenizer used for GPT models also doesn't use an <|unk|> token for outof-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks\n",
    "down words into subword units\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654e4f4",
   "metadata": {},
   "source": [
    "### BYTE PAIR ENCODING (subword based)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3ce206",
   "metadata": {},
   "source": [
    "**BPE Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d724a8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eb3bc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0996cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2d8afdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special= {\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0845211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f32b1",
   "metadata": {},
   "source": [
    "data sampling with sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed62a3e",
   "metadata": {},
   "source": [
    "![Screenshot](images/screenshot1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "24515e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5317\n"
     ]
    }
   ],
   "source": [
    "with open(\"wharton_verdict.txt\", \"r\", encoding= \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c4b6c85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18108, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11, 290, 4920, 2241, 287, 257, 4489, 64, 198, 261, 262, 34686, 41976, 13, 357, 10915, 314, 2138, 1807, 340, 561, 423, 587, 10598, 393, 28537, 2014, 198, 1, 464, 6001, 286, 465, 13476, 1, 438, 5562, 373, 644, 262, 1466]\n",
      "had dropped his painting, married a rich widow, and established himself in a villa\n",
      "on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\"The height of his glory\"--that was what the women\n"
     ]
    }
   ],
   "source": [
    "enc_sample = enc_text[50:100]\n",
    "print(enc_sample)\n",
    "dec_sample = tokenizer.decode(enc_sample)\n",
    "print(dec_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eefff6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [18108, 5710, 465, 12036, 11]\n",
      "y:      [5710, 465, 12036, 11, 6405]\n"
     ]
    }
   ],
   "source": [
    "context_size = 5\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size +1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f405d7",
   "metadata": {},
   "source": [
    "this x and y represent 4 prediction tasks\n",
    "\n",
    "x1 -> y1\n",
    "\n",
    "x1x2 -> y2\n",
    "\n",
    "x1x2x3 -> y3\n",
    "\n",
    "x1x2x3x4 -> y4\n",
    "\n",
    "unlike regression tasks where one input output pair represetns one prediction task\n",
    "\n",
    "\n",
    "here one input output pair represent multiple prediction tasks as set by the context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3f293c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18108] ------> 5710\n",
      "[18108, 5710] ------> 465\n",
      "[18108, 5710, 465] ------> 12036\n",
      "[18108, 5710, 465, 12036] ------> 11\n",
      "[18108, 5710, 465, 12036, 11] ------> 6405\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"------>\", desired)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d8331c",
   "metadata": {},
   "source": [
    "x: [18108, 5710, 465, 12036, 11]\n",
    "y:      [5710, 465, 12036, 11, 6405]\n",
    "\n",
    "the above output can be derived from this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "76d06b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "had ---->  dropped\n",
      "had dropped ---->  his\n",
      "had dropped his ---->  painting\n",
      "had dropped his painting ----> ,\n",
      "had dropped his painting, ---->  married\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6498fdb",
   "metadata": {},
   "source": [
    "### IMPLEMENTING A DATA LOADER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716eb6eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "There's only one more task before we can turn the tokens into embeddings:implementing an efficient data loader that\n",
    "iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which\n",
    "can be thought of as multidimensional arrays.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae03b6ed",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "There's only one more task before we can turn the tokens into embeddings:implementing an efficient data loader that\n",
    "iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which\n",
    "can be thought of as multidimensional arrays.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3b5a8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For the efficient data loader implementation, we will use PyTorch's built-in Dataset and\n",
    "DataLoader classes.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc86712d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Tokenize the entire text\n",
    "    \n",
    "Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "\n",
    "Step 3: Return the total number of rows in the dataset\n",
    "\n",
    "Step 4: Return a single row from the dataset\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde724cc",
   "metadata": {},
   "source": [
    "![Screenshot](images/screenshot2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd1bca",
   "metadata": {},
   "source": [
    "here the input-output target pairs are x1 -> y1, x2 -> ans so on Xn -> Yn\n",
    "\n",
    "each input output pair has multiple prediction tasks dependent on the context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e5e47470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        #using sliding window to ceate input and target pairs\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i+1: max_length + 1 + i]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7747f3",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59afeb",
   "metadata": {},
   "source": [
    "We process token_ids in chunks of 5 tokens (max_length = 5) and move forward by 3 tokens each time (stride = 3).\n",
    "\n",
    "Window 1\n",
    "i = 0\n",
    "\n",
    "input_chunk = token_ids[0:5] = [101, 102, 103, 104, 105]\n",
    "\n",
    "target_chunk = token_ids[1:6] = [102, 103, 104, 105, 106]\n",
    "\n",
    "Window 2\n",
    "i = 3\n",
    "\n",
    "input_chunk = token_ids[3:8] = [104, 105, 106, 107, 108]\n",
    "\n",
    "target_chunk = token_ids[4:9] = [105, 106, 107, 108, 109]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9675bae",
   "metadata": {},
   "source": [
    "https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fe1c72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53327a83",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4, \n",
    "\n",
    "This will develop an intuition of how the GPTDatasetV1 class and the\n",
    "create_dataloader_v1 function work together: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e97621f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wharton_verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8f3f3dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  464, 15593,   198,  7407]]), tensor([[15593,   198,  7407,   342]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size = 1, max_length = 4, stride = 1, shuffle = False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4dbe74",
   "metadata": {},
   "source": [
    "play with strides and batch size to get better understanding also learn how the dataloader and datasets libraries work.\n",
    "\n",
    "https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbDRSY2h3V3lGNUZFWFg0bmtRc3h6QXJTYldaUXxBQ3Jtc0trNlB4V2I5UWxIVDd6VXNEZnNIMkpLLVUxN2hualRyeUM4OS1xUlNjeXBPdXN1bE5xT1ZIMEdFY0U1dGxZdUp3Qzh5Vk8tX054VEFPZkJlZWlfR0RlOWFseVJEUFF3TnpWZHd2QVRwUGpVSnVVTjE1QQ&q=https%3A%2F%2Fpytorch.org%2Ftutorials%2Fbeginner%2Fbasics%2Fdata_tutorial.html&v=iQZFH8dr2yI\n",
    "\n",
    "\n",
    "\n",
    "https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa2JWQXNUd1lEdkV2ckFrWHV5R0Frc29fUUdEQXxBQ3Jtc0trZTJ4bjRQWU1JSkFTMEotaUhpWFVDZ2lXQTY3aUw1ZVBmV2NvTU9Fa24tUk5mMDVjQ0ZNbVoxNmljeGM2T1V0dy1aanJmd2VLem9RTS1HeF9EVlZhTjBqanhocU1MTU9XanU0NDJJZ0hxeWpaR3dFZw&q=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fdata.html&v=iQZFH8dr2yI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90885d8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The first_batch variable contains two tensors: the first tensor stores the input token IDs,\n",
    "and the second tensor stores the target token IDs. \n",
    "\n",
    "Since the max_length is set to 4, each of the two tensors contains 4 token IDs. \n",
    "\n",
    "Note that an input size of 4 is relatively small and only chosen for illustration purposes. It is common to train LLMs with input sizes of at least\n",
    "256.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ec0e5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Batch sizes of 1, such as we have sampled from the data loader so far, are useful for\n",
    "illustration purposes. \n",
    "                                                                                 \n",
    "If you have previous experience with deep learning, you may know\n",
    "that small batch sizes require less memory during training but lead to more noisy model\n",
    "updates.\n",
    "\n",
    "Just like in regular deep learning, the batch size is a trade-off and hyperparameter\n",
    "to experiment with when training LLMs.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13dbce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ea406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1259da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
