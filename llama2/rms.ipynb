{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9376e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab964a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    def __init__(self, head_dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        assert head_dim % 2 == 0, \"head_dim must be even for RoPE\"\n",
    "        self.head_dim = head_dim\n",
    "\n",
    "        # Frequencies for each dimension\n",
    "        theta = 1.0 / (10000 ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        positions = torch.arange(max_seq_len).float().unsqueeze(1)\n",
    "        freqs = positions * theta  # shape: (max_seq_len, head_dim // 2)\n",
    "\n",
    "        # Precompute cos and sin\n",
    "        self.register_buffer(\"cos\", freqs.cos()[None, None, :, :])  # (1, 1, seq_len, head_dim // 2)\n",
    "        self.register_buffer(\"sin\", freqs.sin()[None, None, :, :])  # (1, 1, seq_len, head_dim // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, num_heads, seq_len, head_dim)\n",
    "        b, h, seq_len, d = x.shape\n",
    "\n",
    "        # Separate real and imaginary parts\n",
    "        x1 = x[..., ::2]  # (b, h, seq_len, head_dim // 2)\n",
    "        x2 = x[..., 1::2]  # (b, h, seq_len, head_dim // 2)\n",
    "\n",
    "        cos = self.cos[:, :, :seq_len, :]\n",
    "        sin = self.sin[:, :, :seq_len, :]\n",
    "\n",
    "        # Apply rotation\n",
    "        x_rotated = torch.stack([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        return x_rotated.flatten(-2)  # back to shape: (b, h, seq_len, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30196232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardWithSwiGLU(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, 4 * d_model)\n",
    "        self.linear2 = nn.Linear(4 * d_model, d_model)\n",
    "        self.linear3 = nn.Linear(d_model, 4 * d_model)\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.activation(self.linear1(x) * self.linear3(x)))\n",
    "   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale_weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        norm = torch.sqrt(torch.mean(x ** 2, dim = -1, keepdim = True) + self.eps)\n",
    "        return (x / norm) * self.scale_weight \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMQA(nn.Module):\n",
    "    def __init__(self, d_out, num_heads, d_in, context_length, num_kv_groups):\n",
    "        super().__init__()\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.pos_emb = RoPE(self.head_dim, context_length)\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out)\n",
    "        self.W_key = nn.Linear(d_in, self.head_dim * num_kv_groups)\n",
    "        self.W_value = nn.Linear(d_in, self.head_dim * num_kv_groups)\n",
    "        self.W_out = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x).view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "        values = self.W_value(x).view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "        keys = self.pos_emb(keys)\n",
    "        values = self.pos_emb(values)\n",
    "        queries = self.W_query(x)\n",
    "\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        heads_per_group = self.num_heads // self.num_kv_groups\n",
    "        keys = keys.unsqueeze(2).expand(b, num_tokens, heads_per_group, self.num_kv_groups, self.head_dim)\n",
    "        values = values.unsqueeze(2).expand(b, num_tokens, heads_per_group, self.num_kv_groups, self.head_dim)\n",
    "\n",
    "        keys = keys.reshape(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)     # (b, h, T, d)\n",
    "        values = values.reshape(b, num_tokens, self.num_heads,x self.head_dim).transpose(1, 2) # (b, h, T, d)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(-2, -1) #(b, num_heads, num_tokens, num_tokens)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / (self.head_dim ** 0.5), dim = -1)\n",
    "        attn_output = attn_weights @ values # (b, num_heads, num_tokens, head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(b, num_tokens, self.d_out)\n",
    "        attn_output = self.W_out(attn_output)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb0adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = GMQA(cfg.d_out, cfg.num_heads, cfg.d_in, cfg.context_length, cfg.num_kv_groups)\n",
    "        self.norm1 = RMSNorm(cfg.d_out, eps = cfg.norm_eps)\n",
    "        self.norm2 = RMSNorm(cfg.d_out, eps = cfg.norm_eps)\n",
    "        self.ff = FeedForwardWithSwiglu()\n",
    "\n",
    "    def forward (self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "                          #x = self.drop(x) # dropout is not used in llama models\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x \n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "                           #x = self.drop(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48fcc85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b8a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb5c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb9bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a26687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a7a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11435ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
