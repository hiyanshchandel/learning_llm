{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4883d51",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b9ae30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The verdict\n",
      "Edith wharton\n",
      "\n",
      "I had always thought Jack Gisburn rather a cheap genius--though a good f\n",
      "total number of characters are : 20415\n"
     ]
    }
   ],
   "source": [
    "with open(\"wharton_verdict.txt\",\"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(raw_text[:99])\n",
    "print(f'total number of characters are : {len(raw_text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca8296",
   "metadata": {},
   "source": [
    "Our goal is to tokenize this 20,479-character short story into individual words and special\n",
    "characters that we can then turn into embeddings for LLM training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439231ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello,', ' ', 'world.', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'text', ' ', 'file!!!.', ' ', 'It', ' ', 'contains', ' ', 'some', ' ', 'text.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"hello, world. This is a text file!!!. It contains some text.\"\n",
    "result = re.split(r'(\\s)', text) ## \\s splits wherever thiere is a whitespace\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1868f25",
   "metadata": {},
   "source": [
    "The result is a list of individual words, whitespaces, and punctuation characters\n",
    "\n",
    "\n",
    "Right now the commas and fullstops are part of the words as they dont have white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94c0b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'text', ' ', 'file!!!', '.', '', ' ', 'It', ' ', 'contains', ' ', 'some', ' ', 'text', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a3341",
   "metadata": {},
   "source": [
    "We can see that the words and punctuation characters are now separate list entries just as\n",
    "we wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad772734",
   "metadata": {},
   "source": [
    "A small remaining issue is that the list still includes whitespace characters. Optionally, we\n",
    "can remove these redundant characters safely as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b9fcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '.', 'This', 'is', 'a', 'text', 'file!!!', '.', 'It', 'contains', 'some', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f29c2d",
   "metadata": {},
   "source": [
    "\n",
    "REMOVING WHITESPACES OR NOT\n",
    "\n",
    "\n",
    "When developing a simple tokenizer, whether we should encode whitespaces as\n",
    "separate characters or just remove them depends on our application and its\n",
    "requirements. Removing whitespaces reduces the memory and computing\n",
    "requirements. However, keeping whitespaces can be useful if we train models that\n",
    "are sensitive to the exact structure of the text (for example, Python code, which is\n",
    "sensitive to indentation and spacing). Here, we remove whitespaces for simplicity\n",
    "and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme\n",
    "that includes whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e65ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '.', 'This', 'is', 'a', 'text', 'file', '!', '!', '!', '.', 'It', 'contains', 'some', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.:;\"!?()\\'_]|--|\\s)', text)\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d317c6",
   "metadata": {},
   "source": [
    "Now we apply this basic tokenizer to edith whartons short story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3c24e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_tokenizer(text):\n",
    "    result = re.split(r'([,.:;\"!?()\\'_]|--|\\s)', text)\n",
    "    result = [item for item in result if item.strip()]\n",
    "    print(result[:50])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14c68624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'verdict', 'Edith', 'wharton', 'I', 'had', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow']\n"
     ]
    }
   ],
   "source": [
    "pre_processed = basic_tokenizer(raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf0bb2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total numeber of words : 4650\n"
     ]
    }
   ],
   "source": [
    "print(f'the total numeber of words : {len(pre_processed)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61627c",
   "metadata": {},
   "source": [
    "## Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f0687d",
   "metadata": {},
   "source": [
    "In the previous section, we tokenized Edith Wharton's short story and assigned it to a\n",
    "Python variable called preprocessed. Let's now create a list of all unique tokens and sort\n",
    "them alphabetically to determine the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65475649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'AM', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be', 'Begin', 'Burlington', 'But', 'By', 'Carlo', 'Chicago', 'Claude', 'Come', 'Croft', 'Destroyed', 'Devonshire', 'Don', 'Dubarry', 'Edith', 'Emperors', 'FELT', 'Florence', 'For', 'Gallery', 'Gideon', 'Gisburn', 'Gisburns', 'Grafton', 'Greek', 'Grindle', 'Grindles', 'HAD', 'HAS', 'HAVE']\n",
      "the total number of unique words : 1150\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(pre_processed))\n",
    "vocab_size = len(all_words)\n",
    "print(all_words[:50])\n",
    "print(f'the total number of unique words : {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50f0b28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, \"'\": 2, '(': 3, ')': 4, ',': 5, '--': 6, '.': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'AM': 12, 'Ah': 13, 'Among': 14, 'And': 15, 'Are': 16, 'Arrt': 17, 'As': 18, 'At': 19, 'Be': 20, 'Begin': 21, 'Burlington': 22, 'But': 23, 'By': 24, 'Carlo': 25, 'Chicago': 26, 'Claude': 27, 'Come': 28, 'Croft': 29, 'Destroyed': 30, 'Devonshire': 31, 'Don': 32, 'Dubarry': 33, 'Edith': 34, 'Emperors': 35, 'FELT': 36, 'Florence': 37, 'For': 38, 'Gallery': 39, 'Gideon': 40, 'Gisburn': 41, 'Gisburns': 42, 'Grafton': 43, 'Greek': 44, 'Grindle': 45, 'Grindles': 46, 'HAD': 47, 'HAS': 48, 'HAVE': 49, 'Had': 50, 'Hang': 51, 'Has': 52, 'He': 53, 'Her': 54, 'Hermia': 55, 'His': 56, 'How': 57, 'I': 58, 'If': 59, 'In': 60, 'It': 61, 'Jack': 62, 'Jove': 63, 'Just': 64, 'KNOWN': 65, 'Lord': 66, 'MINE': 67, 'Made': 68, 'Miss': 69, 'Money': 70, 'Monte': 71, 'Moondancers': 72, 'Mr': 73, 'Mrs': 74, 'My': 75, 'NEVER': 76, 'NOT': 77, 'Never': 78, 'No': 79, 'Now': 80, 'Nutley': 81, 'Of': 82, 'Oh': 83, 'On': 84, 'Once': 85, 'Only': 86, 'Or': 87, 'Perhaps': 88, 'Poor': 89, 'Professional': 90, 'RS': 91, 'Renaissance': 92, 'Rickham': 93, 'Riviera': 94, 'Rome': 95, 'Russian': 96, 'Sevres': 97, 'She': 98, 'Stroud': 99, 'Strouds': 100, 'Suddenly': 101, 'THAT': 102, 'THE': 103, 'That': 104, 'The': 105, 'Then': 106, 'There': 107, 'They': 108, 'This': 109, 'Those': 110, 'Though': 111, 'Thwing': 112, 'Thwings': 113, 'To': 114, 'Usually': 115, 'Venetian': 116, 'Victor': 117, 'WAS': 118, 'WERE': 119, 'Was': 120, 'We': 121, 'Well': 122, 'What': 123, 'When': 124, 'Why': 125, 'Yes': 126, 'You': 127, 'a': 128, 'abdication': 129, 'able': 130, 'about': 131, 'above': 132, 'abruptly': 133, 'absolute': 134, 'absorbed': 135, 'absurdity': 136, 'academic': 137, 'accuse': 138, 'accustomed': 139, 'across': 140, 'activity': 141, 'add': 142, 'added': 143, 'admirers': 144, 'adopted': 145, 'adulation': 146, 'advance': 147, 'aesthetic': 148, 'affect': 149, 'afraid': 150, 'after': 151, 'afterward': 152, 'again': 153, 'ago': 154, 'ah': 155, 'air': 156, 'alive': 157, 'all': 158, 'almost': 159, 'alone': 160, 'along': 161, 'always': 162, 'amazement': 163, 'amid': 164, 'among': 165, 'amplest': 166, 'amusing': 167, 'an': 168, 'and': 169, 'another': 170, 'answer': 171, 'answered': 172, 'any': 173, 'anything': 174, 'anywhere': 175, 'apparent': 176, 'apparently': 177, 'appearance': 178, 'appeared': 179, 'appointed': 180, 'are': 181, 'arm': 182, 'arm-chair': 183, 'arm-chairs': 184, 'arms': 185, 'art': 186, 'articles': 187, 'artist': 188, 'as': 189, 'aside': 190, 'asked': 191, 'at': 192, 'atmosphere': 193, 'atom': 194, 'attack': 195, 'attention': 196, 'attitude': 197, 'audacities': 198, 'away': 199, 'awful': 200, 'axioms': 201, 'azaleas': 202, 'back': 203, 'background': 204, 'balance': 205, 'balancing': 206, 'balustraded': 207, 'basking': 208, 'bath-rooms': 209, 'be': 210, 'beaming': 211, 'bean-stalk': 212, 'bear': 213, 'beard': 214, 'beard-as': 215, 'beauty': 216, 'became': 217, 'because': 218, 'becoming': 219, 'bed': 220, 'been': 221, 'before': 222, 'began': 223, 'begun': 224, 'behind': 225, 'being': 226, 'believed': 227, 'beneath': 228, 'bespoke': 229, 'better': 230, 'between': 231, 'big': 232, 'bits': 233, 'bitterness': 234, 'blocked': 235, 'born': 236, 'borne': 237, 'boudoir': 238, 'bravura': 239, 'break': 240, 'breaking': 241, 'breathing': 242, 'bric-a-brac': 243, 'briefly': 244, 'brings': 245, 'bronzes': 246, 'brought': 247, 'brown': 248, 'brush': 249, 'bull': 250, 'business': 251, 'but': 252, 'buying': 253, 'by': 254, 'called': 255, 'came': 256, 'can': 257, 'canvas': 258, 'canvases': 259, 'cards': 260, 'care': 261, 'career': 262, 'caught': 263, 'central': 264, 'chair': 265, 'chap': 266, 'characteristic': 267, 'charming': 268, 'cheap': 269, 'check': 270, 'cheeks': 271, 'chest': 272, 'chimney-piece': 273, 'chucked': 274, 'cigar': 275, 'cigarette': 276, 'cigars': 277, 'circulation': 278, 'circumstance': 279, 'circus-clown': 280, 'claimed': 281, 'clasping': 282, 'clear': 283, 'cleverer': 284, 'close': 285, 'clue': 286, 'coat': 287, 'collapsed': 288, 'colour': 289, 'come': 290, 'comfortable': 291, 'coming': 292, 'companion': 293, 'compared': 294, 'complex': 295, 'confident': 296, 'congesting': 297, 'conjugal': 298, 'constraint': 299, 'consummate': 300, 'contended': 301, 'continued': 302, 'corner': 303, 'corrected': 304, 'cotta': 305, 'could': 306, 'couldn': 307, 'count': 308, 'countenance': 309, 'couple': 310, 'course': 311, 'covered': 312, 'craft': 313, 'cried': 314, 'crossed': 315, 'crowned': 316, 'crumbled': 317, 'cry': 318, 'cured': 319, 'curiosity': 320, 'curious': 321, 'current': 322, 'curtains': 323, 'd': 324, 'dabble': 325, 'damask': 326, 'dark': 327, 'dashed': 328, 'day': 329, 'days': 330, 'dead': 331, 'deadening': 332, 'dear': 333, 'deep': 334, 'deerhound': 335, 'degree': 336, 'delicate': 337, 'demand': 338, 'denied': 339, 'deploring': 340, 'deprecating': 341, 'deprecatingly': 342, 'desire': 343, 'destroyed': 344, 'destruction': 345, 'desultory': 346, 'detail': 347, 'diagnosis': 348, 'did': 349, 'didn': 350, 'died': 351, 'dim': 352, 'dimmest': 353, 'dingy': 354, 'dining-room': 355, 'disarming': 356, 'discovery': 357, 'discrimination': 358, 'discussion': 359, 'disdain': 360, 'disdained': 361, 'disease': 362, 'disguised': 363, 'display': 364, 'dissatisfied': 365, 'distinguished': 366, 'distract': 367, 'divert': 368, 'do': 369, 'doesn': 370, 'doing': 371, 'domestic': 372, 'don': 373, 'done': 374, 'donkey': 375, 'down': 376, 'dozen': 377, 'dragged': 378, 'drawing-room': 379, 'drawing-rooms': 380, 'drawn': 381, 'dress-closets': 382, 'drew': 383, 'dropped': 384, 'each': 385, 'earth': 386, 'ease': 387, 'easel': 388, 'easy': 389, 'echoed': 390, 'economy': 391, 'effect': 392, 'effects': 393, 'efforts': 394, 'egregious': 395, 'eighteenth-century': 396, 'elbow': 397, 'elegant': 398, 'else': 399, 'embarrassed': 400, 'enabled': 401, 'end': 402, 'endless': 403, 'enjoy': 404, 'enlightenment': 405, 'enough': 406, 'ensuing': 407, 'equally': 408, 'equanimity': 409, 'escape': 410, 'established': 411, 'etching': 412, 'even': 413, 'event': 414, 'ever': 415, 'everlasting': 416, 'every': 417, 'exasperated': 418, 'except': 419, 'excuse': 420, 'excusing': 421, 'existed': 422, 'expected': 423, 'exquisite': 424, 'exquisitely': 425, 'extenuation': 426, 'exterminating': 427, 'extracting': 428, 'eye': 429, 'eyebrows': 430, 'eyes': 431, 'face': 432, 'faces': 433, 'fact': 434, 'faded': 435, 'failed': 436, 'failure': 437, 'fair': 438, 'faith': 439, 'false': 440, 'familiar': 441, 'famille-verte': 442, 'fancy': 443, 'fashionable': 444, 'fate': 445, 'feather': 446, 'feet': 447, 'fell': 448, 'fellow': 449, 'felt': 450, 'few': 451, 'fewer': 452, 'finality': 453, 'find': 454, 'fingers': 455, 'first': 456, 'fit': 457, 'fitting': 458, 'five': 459, 'flash': 460, 'flashed': 461, 'florid': 462, 'flowers': 463, 'fluently': 464, 'flung': 465, 'follow': 466, 'followed': 467, 'fond': 468, 'footstep': 469, 'for': 470, 'forced': 471, 'forcing': 472, 'forehead': 473, 'foreign': 474, 'foreseen': 475, 'forgive': 476, 'forgotten': 477, 'form': 478, 'formed': 479, 'forming': 480, 'forward': 481, 'fostered': 482, 'found': 483, 'foundations': 484, 'fragment': 485, 'fragments': 486, 'frame': 487, 'frames': 488, 'frequently': 489, 'friend': 490, 'from': 491, 'full': 492, 'fullest': 493, 'furiously': 494, 'furrowed': 495, 'garlanded': 496, 'garlands': 497, 'gave': 498, 'genial': 499, 'genius': 500, 'gesture': 501, 'get': 502, 'getting': 503, 'give': 504, 'given': 505, 'glad': 506, 'glanced': 507, 'glimpse': 508, 'gloried': 509, 'glory': 510, 'go': 511, 'going': 512, 'gone': 513, 'good': 514, 'good-': 515, 'good-breeding': 516, 'good-humoured': 517, 'got': 518, 'grace': 519, 'gradually': 520, 'gray': 521, 'grayish': 522, 'great': 523, 'greatest': 524, 'greatness': 525, 'grew': 526, 'groping': 527, 'growing': 528, 'had': 529, 'hadn': 530, 'hair': 531, 'half': 532, 'half-light': 533, 'half-mechanically': 534, 'hall': 535, 'hand': 536, 'hands': 537, 'handsome': 538, 'hanging': 539, 'happen': 540, 'happened': 541, 'hard': 542, 'hardly': 543, 'have': 544, 'haven': 545, 'having': 546, 'he': 547, 'head': 548, 'hear': 549, 'heard': 550, 'heart': 551, 'height': 552, 'her': 553, 'here': 554, 'hermit': 555, 'herself': 556, 'hesitations': 557, 'hide': 558, 'high': 559, 'him': 560, 'himself': 561, 'hint': 562, 'his': 563, 'history': 564, 'holding': 565, 'home': 566, 'honour': 567, 'hooded': 568, 'hostess': 569, 'hot-house': 570, 'hour': 571, 'hours': 572, 'house': 573, 'how': 574, 'humoured': 575, 'hung': 576, 'husband': 577, 'idea': 578, 'idle': 579, 'idling': 580, 'if': 581, 'immediately': 582, 'in': 583, 'incense': 584, 'indifferent': 585, 'inevitable': 586, 'inevitably': 587, 'inflexible': 588, 'insensible': 589, 'insignificant': 590, 'instinctively': 591, 'instructive': 592, 'interesting': 593, 'into': 594, 'ironic': 595, 'irony': 596, 'irrelevance': 597, 'irrevocable-the': 598, 'is': 599, 'it': 600, 'its': 601, 'itself': 602, 'jardiniere': 603, 'jealousy': 604, 'just': 605, 'keep': 606, 'kept': 607, 'kind': 608, 'knees': 609, 'knew': 610, 'know': 611, 'laid': 612, 'lair': 613, 'landing': 614, 'language': 615, 'last': 616, 'late': 617, 'late-I': 618, 'later': 619, 'latter': 620, 'laugh': 621, 'laughed': 622, 'lay': 623, 'leading': 624, 'lean': 625, 'learned': 626, 'least': 627, 'leathery': 628, 'leave': 629, 'led': 630, 'left': 631, 'leisure': 632, 'lends': 633, 'lent': 634, 'let': 635, 'lies': 636, 'life': 637, 'life-': 638, 'lift': 639, 'lifted': 640, 'light': 641, 'lightly': 642, 'like': 643, 'liked': 644, 'likeness': 645, 'line': 646, 'lines': 647, 'lingered': 648, 'lips': 649, 'lit': 650, 'little': 651, 'live': 652, 'll': 653, 'loathing': 654, 'long': 655, 'longed': 656, 'longer': 657, 'look': 658, 'looked': 659, 'looking': 660, 'lose': 661, 'loss': 662, 'lounging': 663, 'lovely': 664, 'lucky': 665, 'lump': 666, 'luncheon-table': 667, 'luxury': 668, 'lying': 669, 'made': 670, 'make': 671, 'man': 672, 'manage': 673, 'managed': 674, 'mantel-piece': 675, 'marble': 676, 'married': 677, 'may': 678, 'me': 679, 'meant': 680, 'mediocrity': 681, 'medium': 682, 'mentioned': 683, 'mere': 684, 'merely': 685, 'met': 686, 'might': 687, 'mighty': 688, 'millionaire': 689, 'mine': 690, 'minute': 691, 'minutes': 692, 'mirrors': 693, 'modest': 694, 'modesty': 695, 'moment': 696, 'money': 697, 'monumental': 698, 'mood': 699, 'morbidly': 700, 'more': 701, 'most': 702, 'mourn': 703, 'mourned': 704, 'moustache': 705, 'moved': 706, 'much': 707, 'muddling': 708, 'multiplied': 709, 'murmur': 710, 'muscles': 711, 'must': 712, 'my': 713, 'myself': 714, 'mysterious': 715, 'naive': 716, 'near': 717, 'nearly': 718, 'negatived': 719, 'nervous': 720, 'nervousness': 721, 'neutral': 722, 'never': 723, 'next': 724, 'no': 725, 'no-for': 726, 'none': 727, 'not': 728, 'note': 729, 'nothing': 730, 'now': 731, 'nymphs': 732, 'oak': 733, 'obituary': 734, 'object': 735, 'objects': 736, 'occurred': 737, 'oddly': 738, 'of': 739, 'off': 740, 'off-and': 741, 'often': 742, 'oh': 743, 'old': 744, 'on': 745, 'once': 746, 'one': 747, 'ones': 748, 'only': 749, 'onto': 750, 'open': 751, 'or': 752, 'other': 753, 'our': 754, 'ourselves': 755, 'out': 756, 'outline': 757, 'oval': 758, 'over': 759, 'own': 760, 'packed': 761, 'paid': 762, 'paint': 763, 'painted': 764, 'painter': 765, 'painting': 766, 'pale': 767, 'paled': 768, 'palm-trees': 769, 'panel': 770, 'panelling': 771, 'pardonable': 772, 'pardoned': 773, 'part': 774, 'passages': 775, 'passing': 776, 'past': 777, 'pastels': 778, 'pathos': 779, 'patient': 780, 'people': 781, 'perceptible': 782, 'perfect': 783, 'persistence': 784, 'persuasively': 785, 'phrase': 786, 'picture': 787, 'pictures': 788, 'pines': 789, 'pink': 790, 'place': 791, 'placed': 792, 'plain': 793, 'platitudes': 794, 'pleased': 795, 'pockets': 796, 'point': 797, 'poised': 798, 'poor': 799, 'portrait': 800, 'posing': 801, 'possessed': 802, 'poverty': 803, 'predicted': 804, 'preliminary': 805, 'presenting': 806, 'presses': 807, 'prestidigitation': 808, 'pretty': 809, 'previous': 810, 'price': 811, 'pride': 812, 'princely': 813, 'prism': 814, 'problem': 815, 'proclaiming': 816, 'prodigious': 817, 'profusion': 818, 'protest': 819, 'prove': 820, 'public': 821, 'purblind': 822, 'purely': 823, 'pushed': 824, 'put': 825, 'qualities': 826, 'quality': 827, 'queerly': 828, 'question': 829, 'quickly': 830, 'quietly': 831, 'quite': 832, 'quote': 833, 'rain': 834, 'raised': 835, 'random': 836, 'rather': 837, 're': 838, 'real': 839, 'really': 840, 'reared': 841, 'reason': 842, 'reassurance': 843, 'recovering': 844, 'recreated': 845, 'reflected': 846, 'reflection': 847, 'regrets': 848, 'relatively': 849, 'remained': 850, 'remember': 851, 'reminded': 852, 'repeating': 853, 'represented': 854, 'reproduction': 855, 'resented': 856, 'resolve': 857, 'resources': 858, 'rest': 859, 'rich': 860, 'ridiculous': 861, 'robbed': 862, 'romantic': 863, 'room': 864, 'rose': 865, 'rule': 866, 'run': 867, 's': 868, 'said': 869, 'same': 870, 'satisfaction': 871, 'savour': 872, 'saw': 873, 'say': 874, 'saying': 875, 'says': 876, 'scorn': 877, 'scornful': 878, 'secret': 879, 'see': 880, 'seemed': 881, 'seen': 882, 'self-confident': 883, 'send': 884, 'sensation': 885, 'sensitive': 886, 'sent': 887, 'serious': 888, 'set': 889, 'sex': 890, 'shade': 891, 'shaking': 892, 'shall': 893, 'she': 894, 'shirked': 895, 'short': 896, 'should': 897, 'shoulder': 898, 'shoulders': 899, 'show': 900, 'showed': 901, 'showy': 902, 'shrug': 903, 'shrugged': 904, 'sight': 905, 'sign': 906, 'silent': 907, 'silver': 908, 'similar': 909, 'simpleton': 910, 'simplifications': 911, 'simply': 912, 'since': 913, 'single': 914, 'sitter': 915, 'sitters': 916, 'sketch': 917, 'skill': 918, 'slight': 919, 'slightly': 920, 'slowly': 921, 'small': 922, 'smile': 923, 'smiling': 924, 'sneer': 925, 'so': 926, 'solace': 927, 'some': 928, 'somebody': 929, 'something': 930, 'spacious': 931, 'spaniel': 932, 'speaking-tubes': 933, 'speculations': 934, 'spite': 935, 'splash': 936, 'square': 937, 'stairs': 938, 'stammer': 939, 'stand': 940, 'standing': 941, 'started': 942, 'stay': 943, 'still': 944, 'stocked': 945, 'stood': 946, 'stopped': 947, 'stopping': 948, 'straddling': 949, 'straight': 950, 'strain': 951, 'straining': 952, 'strange': 953, 'straw': 954, 'stream': 955, 'stroke': 956, 'strokes': 957, 'strolled': 958, 'strongest': 959, 'strongly': 960, 'struck': 961, 'studio': 962, 'stuff': 963, 'subject': 964, 'substantial': 965, 'suburban': 966, 'such': 967, 'suddenly': 968, 'suffered': 969, 'sugar': 970, 'suggested': 971, 'sunburn': 972, 'sunburnt': 973, 'sunlit': 974, 'superb': 975, 'sure': 976, 'surest': 977, 'surface': 978, 'surprise': 979, 'surprised': 980, 'surrounded': 981, 'suspected': 982, 'sweetly': 983, 'sweetness': 984, 'swelling': 985, 'swept': 986, 'swum': 987, 't': 988, 'table': 989, 'take': 990, 'taken': 991, 'talking': 992, 'tea': 993, 'tears': 994, 'technicalities': 995, 'technique': 996, 'tell': 997, 'tells': 998, 'tempting': 999, 'terra-': 1000, 'terrace': 1001, 'terraces': 1002, 'terribly': 1003, 'than': 1004, 'that': 1005, 'the': 1006, 'their': 1007, 'them': 1008, 'then': 1009, 'there': 1010, 'therefore': 1011, 'they': 1012, 'thin': 1013, 'thing': 1014, 'things': 1015, 'think': 1016, 'this': 1017, 'thither': 1018, 'those': 1019, 'though': 1020, 'thought': 1021, 'three': 1022, 'threshold': 1023, 'threw': 1024, 'through': 1025, 'throwing': 1026, 'tie': 1027, 'till': 1028, 'time': 1029, 'timorously': 1030, 'tinge': 1031, 'tips': 1032, 'tired': 1033, 'to': 1034, 'told': 1035, 'tone': 1036, 'tones': 1037, 'too': 1038, 'took': 1039, 'tottering': 1040, 'touched': 1041, 'toward': 1042, 'trace': 1043, 'trade': 1044, 'transmute': 1045, 'traps': 1046, 'travelled': 1047, 'tribute': 1048, 'tributes-was': 1049, 'tricks': 1050, 'tried': 1051, 'trouser-': 1052, 'true': 1053, 'truth': 1054, 'turned': 1055, 'twenty': 1056, 'twentyfour': 1057, 'twice': 1058, 'twirling': 1059, 'unaccountable': 1060, 'uncertain': 1061, 'under': 1062, 'underlay': 1063, 'underneath': 1064, 'understand': 1065, 'unexpected': 1066, 'untouched': 1067, 'unusual': 1068, 'up': 1069, 'up-stream': 1070, 'upon': 1071, 'upset': 1072, 'upstairs': 1073, 'us': 1074, 'used': 1075, 'usual': 1076, 'value': 1077, 'varnishing': 1078, 'vases': 1079, 've': 1080, 'veins': 1081, 'velveteen': 1082, 'verdict': 1083, 'very': 1084, 'villa': 1085, 'vindicated': 1086, 'virtuosity': 1087, 'vista': 1088, 'vocation': 1089, 'voice': 1090, 'wall': 1091, 'wander': 1092, 'want': 1093, 'wanted': 1094, 'wants': 1095, 'was': 1096, 'wasn': 1097, 'watched': 1098, 'watching': 1099, 'water-colour': 1100, 'waves': 1101, 'way': 1102, 'weekly': 1103, 'weeks': 1104, 'welcome': 1105, 'went': 1106, 'were': 1107, 'wharton': 1108, 'what': 1109, 'when': 1110, 'whenever': 1111, 'where': 1112, 'which': 1113, 'while': 1114, 'white': 1115, 'white-panelled': 1116, 'who': 1117, 'whole': 1118, 'whom': 1119, 'why': 1120, 'wide': 1121, 'widow': 1122, 'wife': 1123, 'wild': 1124, 'wincing': 1125, 'window-curtains': 1126, 'wish': 1127, 'with': 1128, 'without': 1129, 'wits': 1130, 'woman': 1131, 'women': 1132, 'won': 1133, 'wonder': 1134, 'wondered': 1135, 'word': 1136, 'work': 1137, 'working': 1138, 'worth': 1139, 'would': 1140, 'wouldn': 1141, 'year': 1142, 'years': 1143, 'yellow': 1144, 'yet': 1145, 'you': 1146, 'younger': 1147, 'your': 1148, 'yourself': 1149}\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd41276",
   "metadata": {},
   "source": [
    "As we can see, based on the output above, the dictionary contains individual tokens\n",
    "associated with unique integer labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5642d",
   "metadata": {},
   "source": [
    "Later in this book, when we want to convert the outputs of an LLM from numbers back into\n",
    "text, we also need a way to turn token IDs into text. \n",
    "\n",
    "For this, we can create an inverse\n",
    "version of the vocabulary that maps token IDs back to corresponding text tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8794ca76",
   "metadata": {},
   "source": [
    "Let's implement a complete tokenizer class in Python.\n",
    "\n",
    "The class will have an encode method that splits\n",
    "text into tokens and carries out the string-to-integer mapping to produce token IDs via the\n",
    "vocabulary. \n",
    "\n",
    "In addition, we implement a decode method that carries out the reverse\n",
    "integer-to-string mapping to convert the token IDs back into text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37417a52",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
    "    \n",
    "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
    "\n",
    "Step 3: Process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back into text\n",
    "\n",
    "Step 5: Replace spaces before the specified punctuation\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecf47953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int =vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        pre_processed = re.split(r'([,.:;\"!?()\\'_]|--|\\s)', text)   \n",
    "        pre_processed = [item for item in pre_processed if item.strip()]\n",
    "        ids = [self.str_to_int[word] for word in pre_processed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])   \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)#remove unnecessary spaces before punctuation marks in the string text,   r'\\1'This tells Python to replace the entire match (space + punctuation) with just the punctuation mark (i.e., what's in group 1), removing the space.\n",
    "        return text\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1078ffd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 2, 868, 1006, 616, 547, 764, 5, 1146, 611, 5, 1, 74, 7, 41, 869, 1128, 772, 812, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdf2d906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0e6b2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing\n",
    "text based on a snippet from the training set. \n",
    "\n",
    "Let's now apply it to a new text sample that\n",
    "is not contained in the training set:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "203abb43",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello, world. This is a text file!!!. It contains some text.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m pre_processed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!?()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m_]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)   \n\u001b[1;32m      8\u001b[0m pre_processed \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m pre_processed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m pre_processed]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'hello'"
     ]
    }
   ],
   "source": [
    "text = \"hello, world. This is a text file!!!. It contains some text.\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8fffc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "The problem is that the word \"Hello\" was not used in the The Verdict short story. \n",
    "\n",
    "Hence, it\n",
    "is not contained in the vocabulary. \n",
    "\n",
    "This highlights the need to consider large and diverse\n",
    "training sets to extend the vocabulary when working on LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640cf03",
   "metadata": {},
   "source": [
    "In the previous section, we implemented a simple tokenizer and applied it to a passage\n",
    "from the training set. \n",
    "\n",
    "In this section, we will modify this tokenizer to handle unknown\n",
    "words.\n",
    "\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
    "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
    "<|endoftext|>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e062a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(pre_processed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be9069f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd7485e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1147)\n",
      "('your', 1148)\n",
      "('yourself', 1149)\n",
      "('<|endoftext|>', 1150)\n",
      "('<|unk|>', 1151)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "585f6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        pre_processed = re.split(r'([,.:;\"!?()\\'_]|--|\\s)', text)   \n",
    "        pre_processed = [item for item in pre_processed if item.strip()]\n",
    "        pre_processed = [item if item in self.str_to_int else \"<|unk|>\" for item in pre_processed]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in pre_processed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "acb46d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, do you like tea and cars?<|endoftext|> in the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"hello, do you like tea and cars?\"\n",
    "text2 = \"in the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \"<|endoftext|> \".join([text1, text2])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed784ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1151,\n",
       " 5,\n",
       " 369,\n",
       " 1146,\n",
       " 643,\n",
       " 993,\n",
       " 169,\n",
       " 1151,\n",
       " 10,\n",
       " 1150,\n",
       " 583,\n",
       " 1006,\n",
       " 974,\n",
       " 1002,\n",
       " 739,\n",
       " 1006,\n",
       " 1151,\n",
       " 7]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07ff3e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea and <|unk|>? <|endoftext|> in the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef267ff",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "So far, we have discussed tokenization as an essential step in processing text as input to\n",
    "LLMs. Depending on the LLM, some researchers also consider additional special tokens such\n",
    "as the following:\n",
    "\n",
    "[BOS] (beginning of sequence): This token marks the start of a text. It\n",
    "signifies to the LLM where a piece of content begins.\n",
    "\n",
    "[EOS] (end of sequence): This token is positioned at the end of a text,\n",
    "and is especially useful when concatenating multiple unrelated texts,\n",
    "similar to <|endoftext|>. For instance, when combining two different\n",
    "Wikipedia articles or books, the [EOS] token indicates where one article\n",
    "ends and the next one begins.\n",
    "\n",
    "[PAD] (padding): When training LLMs with batch sizes larger than one,\n",
    "the batch might contain texts of varying lengths. To ensure all texts have\n",
    "the same length, the shorter texts are extended or \"padded\" using the\n",
    "[PAD] token, up to the length of the longest text in the batch.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c172e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that the tokenizer used for GPT models does not need any of these tokens mentioned\n",
    "above but only uses an <|endoftext|> token for simplicity\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d3eca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "the tokenizer used for GPT models also doesn't use an <|unk|> token for outof-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks\n",
    "down words into subword units\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654e4f4",
   "metadata": {},
   "source": [
    "### BYTE PAIR ENCODING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3ce206",
   "metadata": {},
   "source": [
    "**BPE Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d724a8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb3bc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0996cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d8afdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special= {\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0845211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f32b1",
   "metadata": {},
   "source": [
    "data sampling with sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "24515e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5317\n"
     ]
    }
   ],
   "source": [
    "with open(\"wharton_verdict.txt\", \"r\", encoding= \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4b6c85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 15593, 198, 7407, 342, 348, 41328, 198, 198, 40, 550, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 198, 48229, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 198]\n",
      "The verdict\n",
      "Edith wharton\n",
      "\n",
      "I had always thought Jack Gisburn rather a cheap genius--though a good fellow\n",
      "enough--so it was no great surprise to me to hear that, in the height of his glory, he\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enc_sample = enc_text[:50]\n",
    "print(enc_sample)\n",
    "dec_sample = tokenizer.decode(enc_sample)\n",
    "print(dec_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefff6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
